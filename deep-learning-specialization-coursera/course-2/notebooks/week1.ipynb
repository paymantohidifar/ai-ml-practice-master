{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9e06a3-800f-452f-9a4b-226ba72c6d98",
   "metadata": {},
   "source": [
    "# Week 1: Practical Aspects of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba5053-de56-4e91-b5ab-4eadca0f6a1e",
   "metadata": {},
   "source": [
    "### Train/Dev/Test sets\n",
    "\n",
    "![Train/Dev/Test sets](images/train_dev_test.png)\n",
    "\n",
    "A few rules of thumb:\n",
    "- Make sure dev and test come from same distribution that your train set comes from.\n",
    "- Not having a test set **might** be okay (only dev set). Although not recommended, if a more accurate measure of the performance is not necessary it is ok to not use a test set. However, this might cause an overfit to the dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69045bb1-a04f-4851-9f0a-8d92ccf3dd4b",
   "metadata": {},
   "source": [
    "### Bias and variance\n",
    "\n",
    "![Bias and Variance](images/bias_variance1.png)\n",
    "\n",
    "How to evaluate bias and variance of a model?\n",
    "\n",
    "| Error | Scenario 1 | Scenario 2 | Scenario 3 | Scenario 4 |\n",
    "| ----- | ---------- | ---------- | ---------- | ---------- |\n",
    "| Train set error | 1 % | 15% | 15% | 0.5%|\n",
    "| Dev set error | 11 % | 16% | 30% | 1.0% |\n",
    "| Evaluation | High var. | High bias | High bias & high var. | Low bias & low var. |\n",
    "\n",
    "Actually, above evaluations were based on an **optimal (Bayes) error $\\sim$ 0**. These evaluations can change if Bayes error is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291f96b-68f1-44f8-8f87-12508aff0b57",
   "metadata": {},
   "source": [
    "### Basic Recipe for Machine Learning\n",
    "\n",
    "![Recipe for ML](images/recipe_for_ml.png)\n",
    "\n",
    "Previously, the big challenges was the trade-off between bias and variance, meaning that low bias was leading to high varinace and vice versa. However, in modern ML, we seem to have solved the problem by having access to more data or better and algorithms/computing power. That's why deep learning is working well for supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648c65d-03cb-421c-b320-63b09e907a00",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "To reduce overfitting or variance, one way is to train the model on more data. However, access to more data is often expensive. Regularization can help here.\n",
    "\n",
    "#### Logistic Regression Regularization\n",
    "\n",
    "![Logestic regression regularization](images/lreg_reg.png)\n",
    "\n",
    "Note that in common regularization practices L2 norm is used. However, some use L1 norm because they believe L1 norm produces sparse matrix which help with computation speed. However, the improvment is subtle in practice and L2 norm is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be0e60-e224-4a70-905e-744e9073a650",
   "metadata": {},
   "source": [
    "#### Neural Network L2 (Frobenious) Regularization\n",
    "\n",
    "![Neural Network regularization](images/nn_reg.png)\n",
    "\n",
    "**Correction:**\n",
    "\n",
    "In above formulation of Forbenius norm, the correct formula is the following:\n",
    "\n",
    "$$\n",
    "||w^{[l]}||^2 = \\sum_{i=1}^{n^{[l]}} \\sum_{j=1}^{n^{[l-1]}} (w_{i,j}^{[l]})^2\n",
    "$$\n",
    "\n",
    "* The limit of summation of i should be from 1 to $n^{[l]}$.\n",
    "* The limit of summation of j should be from 1 to $n^{[l−1]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5eb555-42f1-47a5-bfba-2eeff941747b",
   "metadata": {},
   "source": [
    "### Why does regularization prevent overfitting (variance)?\n",
    "\n",
    "![NN regularization intuition1](images/nn_reg_intuition1.png)\n",
    "\n",
    "![NN regularization intuition2](images/nn_reg_intuition2.png)\n",
    "\n",
    "Regularization also push parameter to very small numbers. This would eliminate some of units or hidden layers, which would reduce overfitting. In addition, smaller parameters drive down $Z$ values. $tanh()$ activation function would be linear for smaller $Z$s. This would in turn make hidden layers linear and linear hidden layers will make the whole NN close to linear function and reduced overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906f40d-63b7-463e-aa69-91873c699219",
   "metadata": {},
   "source": [
    "### Dropout Regularization\n",
    "\n",
    "The core idea of dropout is to randomly \"drop out\" (set to zero) a certain percentage of neurons in a layer during each training iteration. This means that for each training example, the network has a slightly different architecture because a different set of neurons is temporarily removed.\n",
    "\n",
    "1.  **During Training:**\n",
    "    * A dropout rate, typically between 20% and 50%, is chosen. This is the probability that a neuron's output will be set to zero.\n",
    "    * For each training batch, a new, random set of neurons is \"dropped out\" from the specified layers. This effectively creates a \"thinned\" version of the network for that particular training step.\n",
    "    * The remaining, active neurons are scaled up by a factor of $1/(1 - \\text{dropout rate})$. This is known as **inverted dropout** and ensures the expected output of the layer remains consistent, which is crucial for a stable learning process.\n",
    "\n",
    "2.  **During Inference (Testing):**\n",
    "    * Dropout is **not** applied during testing. All neurons are active and used for making predictions.\n",
    "    * Because the neuron activations were scaled during training, no additional scaling is needed during inference. The network's output is already on the correct scale, which makes prediction a simpler and faster process.\n",
    "\n",
    "| | **Standard Dropout** | **Inverted Dropout** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Training** | Drop out neurons with probability **$p$**. | Drop out neurons with probability **$p$**, and scale remaining activations by multiplying by **$1/(1-p)$**. |\n",
    "| **Inference** | Use all neurons, and scale their outputs by multiplying by **$(1-p)$**. | Use all neurons, with **no scaling** required. |\n",
    "\n",
    "---\n",
    "\n",
    "This video provides an explanation of dropout, a key regularization technique in deep learning:\n",
    "\n",
    "<u>[Dropout: A Method to Regularize the Training of Deep Neural Networks](https://www.youtube.com/watch?v=hbTONDwwPUk)</u>\n",
    "\n",
    "#### Why It Helps\n",
    "\n",
    "Dropout works by preventing neurons from becoming too specialized and co-dependent on each other. By forcing neurons to learn to function in a wide variety of different network configurations, it encourages the network to learn more robust and generalizable features. This is analogous to having multiple separate, smaller neural networks being trained simultaneously and then averaged together, a technique known as **ensemble learning**. The result is a single, more resilient model that is less likely to overfit the training data.\n",
    "\n",
    "#### Bonus\n",
    "\n",
    "This video provides a theoretical and practical explanation of dropout regularization, including an implementation using TensorFlow and Keras.\n",
    "\n",
    "<u>[What is Dropout Regularization?](https://www.youtube.com/watch?v=lcI8ukTUEbo)</u>\n",
    "\n",
    "---\n",
    "![dropout regularization](images/dropout_reg.png)\n",
    "\n",
    "---\n",
    "\n",
    "![dropout regularization](images/inverted_dropout_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28bb74-d540-4359-9c8f-1af03771600b",
   "metadata": {},
   "source": [
    "### Understanding dropout\n",
    "\n",
    "#### Why does dropout work?\n",
    "\n",
    "* **Prevents Co-dependence:** Dropout randomly \"knocks out\" neurons during each training iteration. This forces the remaining neurons to learn more robust and independent features because they cannot rely on any single neuron, as that neuron might be dropped out.\n",
    "* **Shrinks Weights:** By forcing the network to spread out the \"responsibility\" for learning across more neurons, dropout has a similar effect to L2 regularization, which shrinks the weights and helps prevent overfitting.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "* **Varying Dropout Rates:** You can set different \"keep probabilities\" (the probability a neuron is kept) for different layers.\n",
    "    * Layers with a large number of parameters are more prone to overfitting, so you can apply a lower keep probability (e.g., 0.5) to these layers for stronger regularization.\n",
    "    * Layers that are less likely to overfit can have a higher keep probability (e.g., 0.7) or even 1.0 (no dropout).\n",
    "    * The input layer typically has a keep probability close to 1.0.\n",
    "\n",
    "#### Practical Considerations\n",
    "\n",
    "* **When to Use It:** Dropout is a regularization technique. It should only be used when a model is overfitting. Computer vision models, which often have a massive number of input features (pixels), frequently use dropout because they are prone to overfitting due to limited data.\n",
    "* **Debugging:** Dropout makes the cost function ($J$) less well-defined because the network architecture changes with each iteration. This makes it difficult to use the cost function to debug your code. A common practice is to first run the model with dropout turned off (keep probability = 1.0) to ensure the cost function is decreasing, and then re-enable dropout.\n",
    "\n",
    "---\n",
    "\n",
    "![why does dropout work](images/why_dropout_works.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d92b1-6caf-4d79-a9d8-26264ca40c5c",
   "metadata": {},
   "source": [
    "### Other Regularization Methods\n",
    "\n",
    "#### Data Augmentation\n",
    "* **The Concept:** When you don't have enough training data to prevent overfitting, you can create new, synthetic data by applying transformations to your existing training examples.\n",
    "* **How It Works:** For images, you can create new examples by flipping images horizontally, taking random crops, or applying slight distortions. For other data like optical character recognition, you can apply rotations and distortions to digits.\n",
    "* **Benefits:** This method is a very inexpensive way to increase the size of your training set and effectively \"regularize\" your model, telling it that these transformations don't change the underlying subject (e.g., a flipped cat is still a cat).\n",
    "\n",
    "---\n",
    "\n",
    "#### Early Stopping\n",
    "* **The Concept:** This technique involves stopping the training process before the model has fully converged to its lowest training error.\n",
    "* **How It Works:** You monitor both the training error and the dev set (validation) error during gradient descent. The training error will likely decrease monotonically, but the dev set error will decrease for a while and then start to increase as the model begins to overfit. Early stopping stops the training at the point where the dev set error is at its minimum.\n",
    "* **Why It Works:** Early stopping effectively finds a model with a smaller parameter size (W) since it hasn't had as much time to train and grow large. This has a similar effect to L2 regularization, which also penalizes large weights.\n",
    "* **Trade-offs:** Early stopping couples two separate machine learning goals: optimizing the cost function and preventing overfitting. This can make the process of tuning hyperparameters more complicated. An alternative is to use L2 regularization and train the model for a longer period, which decouples these two tasks but can be more computationally expensive (because we will need to tune regularization $\\lambda$ hyperparameter using cross-validation).\n",
    "\n",
    "---\n",
    "\n",
    "![Early stopping](images/early_stopping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f959026-492f-48a9-9b1c-5ff47370bb19",
   "metadata": {},
   "source": [
    "### Setting up your optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0daf9a-8bb0-4f74-b3d3-63f09f9b4348",
   "metadata": {},
   "source": [
    "#### Normalizing inputs\n",
    "\n",
    "Normalizing input features is a crucial step to speed up a neural network's training. It involves two steps:\n",
    "\n",
    "* **Zero the Mean:** Subtract the mean of each feature from its values so the new mean is zero.\n",
    "* **Normalize the Variance:** Divide each feature's values by its variance, ensuring all features have a similar range.\n",
    "\n",
    "This process makes the **cost function's** contours more symmetric and spherical rather than elongated. A more balanced cost function allows the **gradient descent** algorithm to converge much faster and more efficiently by taking larger, more direct steps toward the minimum instead of many small, oscillating steps.\n",
    "\n",
    "It's important to use the same mean and variance calculated from the **training set** to normalize the **test set** as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7846abe-436d-4abb-9ce5-d2a64bbabc44",
   "metadata": {},
   "source": [
    "#### Vanishing/Exploding Gradients\n",
    "\n",
    "When training deep neural networks, **vanishing and exploding gradients** are problems where the gradients—used to update the network's weights—become either extremely small or extremely large, making training difficult.\n",
    "\n",
    "* **Exploding Gradients**: Occur when weights are slightly greater than one, causing gradients to grow exponentially with each layer.\n",
    "* **Vanishing Gradients**: Occur when weights are slightly less than one, causing gradients to shrink exponentially with each layer. This is especially problematic as it slows down learning.\n",
    "\n",
    "The primary solution is a careful choice of **random weight initialization**, which helps to prevent the gradients from becoming too large or too small, allowing the network to train more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "![Vanishing Exploding Gradients](images/exploding_vanishing_gradients.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce169f9-deaf-480e-894d-9e40e2c542fd",
   "metadata": {},
   "source": [
    "#### Weight Initialization for Deep Networks\n",
    "\n",
    "* **The Goal of Weight Initialization:** The main objective is to initialize the weights so that they are neither too large nor too small. If the weights are too large, the activations and gradients can grow exponentially (**exploding gradients**). If they are too small, the activations and gradients can shrink exponentially toward zero (**vanishing gradients**), which stops the network from learning.\n",
    "\n",
    "* **Initialization for a Single Neuron:** For a single neuron with many input features, the sum of the products of weights and inputs ($z$) can become very large. To keep $z$ on a reasonable scale, the variance of the weights should be inversely proportional to the number of input features ($n$). A common approach is to set the variance of the weights ($W$) to be $1/n$.\n",
    "\n",
    "* **Generalizing to Deep Networks:**\n",
    "    * **ReLU Activation:** For a network using the **ReLU activation function** (the most common choice), a good practice is to set the variance of the weights for a given layer to $2/n_{in}$, where $n_{in}$ is the number of input features to that layer. The weights are then initialized from a random distribution (e.g., a standard normal distribution) and scaled by $\\sqrt{2/n_{in}}$.\n",
    "    * **Tanh Activation:** For a network using the **Tanh activation function**, a slightly different approach, known as **Xavier initialization**, is often used. This method scales the weights by $\\sqrt{1/n_{in}}$. Other variants, such as one by Bengio et al., also exist.\n",
    "\n",
    "* **Practical Application:** In practice, using these initialization formulas provides a strong starting point for training. While the specific constants (like 1 or 2) can be treated as hyperparameters to be fine-tuned, these standard values are generally very effective at mitigating the vanishing and exploding gradient problems, allowing for <u>faster and more stable training of deep networks</u>.\n",
    "   \n",
    "**What you should remember**:\n",
    "- The weights $W^{[l]}$ should be initialized randomly to break symmetry. \n",
    "- However, it's okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly. \n",
    "\n",
    "---\n",
    "\n",
    "![Weight Initialization](images/param_initialization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763dcabf-0f6b-4c21-bd68-558f4b0c3aaa",
   "metadata": {},
   "source": [
    "#### Numerical Approximation of Gradients\n",
    "\n",
    "Gradient checking is a powerful technique for debugging and verifying the correctness of a neural network's backpropagation implementation. It relies on numerically approximating the gradient and comparing it to the gradient calculated by backpropagation.\n",
    "\n",
    "##### **The Two-Sided Difference Method**\n",
    "To numerically approximate a gradient, you use the two-sided difference formula, which provides a much more accurate estimate than the one-sided method.\n",
    "\n",
    "* **Formula**: The approximation of the derivative of a function $f(\\theta)$ is calculated as:\n",
    "    $\\frac{f(\\theta + \\epsilon) - f(\\theta - \\epsilon)}{2\\epsilon}$\n",
    "\n",
    "    Here, $\\theta$ is the parameter you're checking, and $\\epsilon$ is a very small number (e.g., $0.01$).\n",
    "* **Intuition**: Instead of measuring the slope on just one side of a point, this method measures the slope of a line that passes through two points equidistant from $\\theta$. This \"two-sided\" approach effectively averages the slopes on either side, providing a much more precise approximation of the true derivative at that point. \n",
    "\n",
    "##### **Why It's Better**\n",
    "The two-sided difference method is far more accurate than the one-sided method ($\\frac{f(\\theta + \\epsilon) - f(\\theta)}{\\epsilon}$). While the one-sided error is proportional to $\\epsilon$, the two-sided error is proportional to $\\epsilon^2$. Since $\\epsilon$ is a very small number, $\\epsilon^2$ is even smaller (e.g., if $\\epsilon = 0.01$, $\\epsilon^2 = 0.0001$), resulting in a much more precise approximation.\n",
    "\n",
    "##### **How it's Used**\n",
    "* **Backpropagation Check**: To perform gradient checking, you first use your backpropagation implementation to compute the gradient of your cost function with respect to your model's parameters.\n",
    "* **Numerical Check**: Then, you use the two-sided difference formula to numerically approximate the same gradient.\n",
    "* **Comparison**: You compare the two results. If they are very close, you can be confident that your backpropagation code is correct. If there's a significant difference, it indicates a bug in your backpropagation implementation.\n",
    "\n",
    "---\n",
    "\n",
    "![Derivative Approximation](images/derivative_approximation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98c20b-55ee-4860-93d4-edc0801cd5f0",
   "metadata": {},
   "source": [
    "#### Gradient Checking\n",
    "\n",
    "Gradient checking is a powerful debugging tool used to verify that the implementation of **backpropagation** is correct. Since backpropagation involves complex derivative calculations, it's easy to make subtle errors. Gradient checking provides a way to numerically approximate the gradient and compare it to the one calculated by your code.\n",
    "\n",
    "##### **The Procedure**\n",
    "1.  **Reshape Parameters:** First, take all the parameters of your neural network (**$W$** and **$b$** for all layers) and reshape them into a single, giant vector called **$\\theta$**.\n",
    "2.  **Reshape Gradients:** Similarly, take all the gradients calculated by backpropagation (**$dW$** and **$db$**) and reshape them into a single vector called **$d\\theta$**. This vector should have the same dimensions as **$\\theta$**.\n",
    "3.  **Numerical Approximation Loop:** For each element $\\theta_i$ in the vector **$\\theta$**, use the **two-sided difference** method to numerically approximate its gradient. This is done by calculating:\n",
    "    $$\n",
    "    \\frac{J(\\theta_1, ..., \\theta_i+\\epsilon, ...) - J(\\theta_1, ..., \\theta_i-\\epsilon, ...)}{2\\epsilon}\n",
    "    $$\n",
    "    This process is repeated for every element in **$\\theta$**, creating a new vector of approximated gradients, **$d\\theta_{approx}$**.\n",
    "5.  **Compare the Gradients:** Finally, compare the numerically approximated gradient vector ($d\\theta_{approx}$) to the one calculated by your backpropagation code ($d\\theta$). A good way to measure the difference is to use the following formula:\n",
    "\n",
    "    $$\n",
    "    \\frac{\\|d\\theta_{approx} - d\\theta\\|_2}{\\|d\\theta_{approx}\\|_2 + \\|d\\theta\\|_2}\n",
    "    $$\n",
    "\n",
    "    This formula calculates the Euclidean distance between the two vectors and normalizes it by their combined lengths.\n",
    "\n",
    "##### **Interpreting the Results**\n",
    "* **Result is < $10^{-7}$**: This is an excellent result and strongly suggests that your backpropagation implementation is correct.\n",
    "* **Result is around $10^{-5}$**: This might be acceptable, but you should double-check your code, as there could be a small bug.\n",
    "* **Result is > $10^{-3}$**: This indicates a significant problem, and it's almost certain that there is a bug in your backpropagation code. You should debug your implementation until this value becomes very small.\n",
    "\n",
    "By using gradient checking, you can quickly find and fix bugs that would otherwise be very difficult to locate, saving a great deal of time and effort. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0ed3b-cae4-4597-9cdb-48f69608a78a",
   "metadata": {},
   "source": [
    "#### Gradient Checking Implementation\n",
    "\n",
    "Here are some key tips for effectively implementing and using **gradient checking** to debug your neural network.\n",
    "\n",
    "* **Use for Debugging Only:** Gradient checking is computationally very slow, so you should **only use it for debugging** your backpropagation code. After you've confirmed your backpropagation is correct, you should turn off gradient checking and rely solely on backpropagation for training.\n",
    "\n",
    "* **Pinpoint the Bug:** If the gradient check fails, examine the individual components of the numerically approximated gradient vector and the backpropagation gradient vector. By identifying which specific parameters (e.g., a specific layer's **$W$** or **$b$** values) have a large discrepancy, you can narrow down the location of the bug in your code.\n",
    "\n",
    "* **Include Regularization:** If your cost function includes a regularization term (e.g., L2 regularization), remember to include this term when computing the cost during gradient checking. The gradient you're checking against must also include the derivative of the regularization term.\n",
    "\n",
    "* **Don't Use with Dropout:** Gradient checking is not compatible with **dropout** because dropout randomly deactivates neurons in each iteration. This means the cost function is not fixed but is constantly changing, making it impossible to perform a stable numerical approximation. To use gradient checking, you should temporarily turn off dropout (by setting the keep probability to 1.0) and verify your code without it. Then, you can turn dropout back on.\n",
    "\n",
    "* **Test at Multiple Points:** It is rare, but sometimes a bug in backpropagation only appears when the weights (**$W$** and **$b$**) are far from their initial small values. To be thorough, you can run a gradient check not only at the beginning of training (with random initialization) but also after training for a number of iterations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
