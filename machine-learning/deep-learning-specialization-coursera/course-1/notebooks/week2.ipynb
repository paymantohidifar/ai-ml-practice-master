{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11aca4df-404c-4a78-9ffa-bc9e94d8b778",
   "metadata": {},
   "source": [
    "# Week 2: Neural Networks Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e81e50-429c-4e3f-b9f9-f32ae5d6ae5b",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "Here we want to predict what a dataset can be classify as. For example, we want to predict if an image is a cat or not. Here is the notation of the problem:\n",
    "\n",
    "![binary classification notation](images/bin_class_notation.png)\n",
    "\n",
    "## Logistic classification\n",
    "\n",
    "![logistic regression](images/log_reg.png)\n",
    "\n",
    "\n",
    "### Loss (error) function\n",
    "\n",
    "In logistic regression, we could use squared error ($\\frac{1}{2}(\\hat y - y)^2$) as our loss function. However, this function won't be convex and gradient descent won't yield a global minumum. To have a convex loss function, we will be using the following formulation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(\\hat y, y) = -(ylog{(\\hat y)} + (1-y)log{(1-\\hat y)})\n",
    "\\end{equation}\n",
    "\\label{eq: log loss}\n",
    "$$\n",
    "\n",
    "Here is how loss function is derived:\n",
    "\n",
    "![logistic reg loss derivation](images/lreg_loss_derivation.png)\n",
    "\n",
    "\n",
    "**Key Difference: Loss vs. Cost Function**\n",
    "\n",
    "While the terms are often used interchangeably, there's a subtle but important distinction.\n",
    "\n",
    "- **Loss Function:** Measures the error for a single training example.\n",
    "- **Cost Function:** Measures the average error across the entire training dataset. It's the value that you ultimately seek to minimize to train your model.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "J(w,b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat y^{(i)}, y^{(i)})\n",
    "\\end{equation}\n",
    "\\label{eq: cost function}\n",
    "$$\n",
    "\n",
    "And, here is how loss function is derived, assuming training examples are i.i.d. (identically independently distributed):\n",
    "\n",
    "![logistic reg cost derivation](images/lreg_cost_derivation.png)\n",
    "\n",
    "\n",
    "### Computation graph\n",
    "\n",
    "The coding convention `dvar` represent the derivative of a final output variable with respect to various intermediate quantities. For example, $\\frac{\\partial J}{\\partial a}$ is denoted as $da$.\n",
    "\n",
    "### Logistic Regression Gradient Descent\n",
    "\n",
    "Here is th algorithm's formulation for 1 training example:\n",
    "\n",
    "![logistic reg gradient descent](images/lreg_gd.png)\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "w_1 &:= w_1 - \\alpha (a-y) x_1 \\\\\n",
    "w_2 &:= w_2 - \\alpha (a-y) x_2 \\\\\n",
    "b   &:= b -   \\alpha (a-y)\n",
    "\\end{aligned}\n",
    "\\label{eq:lreg_gd_1m}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In the case of m training examples, we can rewrite the cost gradient descent algorithm using the cost function $J(w,b)$:\n",
    "\n",
    "![logistic reg gradient descent general](images/lreg_gd_general.png)\n",
    "\n",
    "As you see above, we need to implement two for loops: one for $m$ training examples, another for $n$ features. In deep learning, we need to deal with large set of features and training examples. For loops will make the algorithm very very slow. That's why we need to use **vectorization**.\n",
    "\n",
    "Here is the vectorized format of the above implementation:\n",
    "\n",
    "![logistic reg gradient descent general vectorized](images/lreg_gd_general_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3667018-4798-4e07-a7a9-551fd40fabf6",
   "metadata": {},
   "source": [
    "## Bonus: A few notes on Python's Numpy\n",
    "\n",
    "- When you aim to use vector operations, it's recommended to convert rank 1 array into n-dimensional array (or vectors). You can do this using Numpy's `reshape()` function.\n",
    "- Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing $x$ to $\\frac{ùë•}{‚Äñùë•‚Äñ}$ (dividing each row vector of $x$ by its 2-norm). We can do this using:\n",
    "\n",
    "    ```Python\n",
    "    np.linalg.norm(x, axis=1, keepdims=True, ord=2)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5ac500-fc27-4a80-a931-1f550f6b5d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.6       , 0.8       ],\n",
       "       [0.26726124, 0.80178373, 0.53452248]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[0, 3, 4], [2, 6, 4]])\n",
    "x / np.linalg.norm(x, axis=1, keepdims=True, ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0250e37-ac39-4cf0-8095-964b0358644d",
   "metadata": {},
   "source": [
    "## Bonus: Softmax\n",
    "\n",
    "You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.\n",
    "\n",
    "**Instructions**:\n",
    "- for $x \\in \\mathbb{R}^{1\\times n}$,\n",
    "\n",
    "\\begin{align*}\n",
    " softmax(x) &= softmax\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "\n",
    "- for a matrix $x \\in \\mathbb{R}^{m \\times n}$, $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have:\n",
    "\n",
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d97c1d-e794-4949-bbe0-8afcf42e2944",
   "metadata": {},
   "source": [
    "## Bonus: `copy.deepcopy()`\n",
    "\n",
    "The line of Python code `w = copy.deepcopy(w)` creates a **completely independent copy** of the object `w`.\n",
    "\n",
    "Let's break down why this is important and what it does:\n",
    "\n",
    "* **What is `copy.deepcopy()`?**\n",
    "    * Python's `copy` module provides functions for creating copies of objects.\n",
    "    * `copy.deepcopy()` performs a **deep copy**. This means it not only copies the object itself but also recursively copies all objects contained within it.\n",
    "\n",
    "* **Why use `deepcopy` instead of a simple assignment (`w = w`) or a shallow copy (`w = copy.copy(w)`)?**\n",
    "\n",
    "    * **Simple Assignment (`w = w`):** This doesn't create a copy at all. It simply makes another variable name (`w`) point to the *exact same object* in memory. Any changes made through either variable will affect the other because they are referencing the identical object.\n",
    "\n",
    "    * **Shallow Copy (`w = copy.copy(w)`):** A shallow copy creates a new object, but it inserts references into the new object to the *original objects* found in the original. If the original object contains other mutable objects (like lists or dictionaries), the shallow copy will still share those nested objects with the original. Changing a nested mutable object in the copy will also change it in the original, and vice-versa.\n",
    "\n",
    "    * **Deep Copy (`w = copy.deepcopy(w)`):** This is where `deepcopy` shines. It creates a new compound object and then, recursively, inserts *copies* of the objects found in the original into the new one. This ensures that the new object and all its nested objects are entirely separate from the original. Changes made to the deep copy will **never** affect the original object, and vice-versa.\n",
    "\n",
    "**In essence, `w = copy.deepcopy(w)` is used when you need to:**\n",
    "\n",
    "1.  **Modify a complex data structure** (like a list of lists, a dictionary of dictionaries, or custom objects) without altering the original.\n",
    "2.  **Preserve the original state** of an object while experimenting with modifications on a separate version.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "* **Assignment:** Giving someone a key to your house. They can change anything inside.\n",
    "* **Shallow Copy:** Making a photocopy of a binder. The pages are new, but if a page contains a smaller, nested binder, that smaller binder is still the original one.\n",
    "* **Deep Copy:** Making a photocopy of a binder, and then photocopying *every single page* and *every single item* within any nested binders, creating entirely new, separate copies of everything. üìÑ\n",
    "\n",
    "This is a very common and useful technique in programming to prevent unintended side effects when working with mutable data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b9b41-97d3-4767-a61e-5818e1cd0d4f",
   "metadata": {},
   "source": [
    "- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting.\n",
    "- It happens when the training accuracy is a lot higher than the test accuracy.\n",
    "- In deep learning, we usually recommend that you:\n",
    "    - Choose the learning rate that better minimizes the cost function.\n",
    "    - If your model overfits, use other techniques to reduce overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
