{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794b6134-50a0-4dd7-bf61-ac6b0b1a1a8f",
   "metadata": {},
   "source": [
    "# Week 3: Shallow Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afee32-5238-4365-aa8a-e3f88454dbd2",
   "metadata": {},
   "source": [
    "![Neural Network Rep1](images/nn_representation.png)\n",
    "\n",
    "---\n",
    "![Neural Network Rep2](images/nn_representation2.png)\n",
    "\n",
    "---\n",
    "![Neural Network Rep3](images/nn_representation3.png)\n",
    "\n",
    "---\n",
    "![Neural Network Rep4](images/nn_representation4.png)\n",
    "\n",
    "---\n",
    "![Neural Network Rep5](images/nn_representation5.png)\n",
    "\n",
    "---\n",
    "![Activation functions1](images/activation_funcs1.png)\n",
    "\n",
    "---\n",
    "![Activation functions2](images/activation_funcs2.png)\n",
    "\n",
    "Regarding the activation functions, we almost never use sigmoid function in hiddeb layers. The reason is the mean of nodes in a given layer is not zero and often this slows down the algorithm. Instead, we will use $tanh$ function, which has a mean of zero, or more often ReLu fucntion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa2630-906a-4079-9b83-5d63d85c6d71",
   "metadata": {},
   "source": [
    "## Why do you need Non-linear activation functions?\n",
    "\n",
    "If we use a linear function as the activarion function in hidden layers, we will end of having a linear function at the end. This will make the hidden layers redundant. The only layer that we can use linear activation function is output layer in regression models. Even in these cases, a better choice would be a ReLu function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762bbbf2-a3d2-44d0-b9be-da8a9eb1ef6b",
   "metadata": {},
   "source": [
    "## Neural Network gradient\n",
    "![gradient descent derivative](images/gd_derivative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95f70c-8a5b-4e20-8936-5bda0b83c6e6",
   "metadata": {},
   "source": [
    "## Symmary of Gradient Descent for Neural Networks\n",
    "\n",
    "![gradient descent derivative summary](images/gd_derivative_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35020f5f-5f7a-4ee5-9cb9-11fba899b78a",
   "metadata": {},
   "source": [
    "### Importance of Random Initialization\n",
    "\n",
    "![random initilazation](images/parameters_rand_init.png)\n",
    "\n",
    "In the case of NN, initialization of parameters to zero will result in same units on hidden layers. That's why we need to assign small random numbers. The reason we assign *small* numbers is to increase convergence rate of gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2ff01-2b3a-4653-8276-63840f10efbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
