{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1bab82f-606d-4812-9f0d-347932f60d18",
   "metadata": {},
   "source": [
    "# Introduction to gradients and automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc882b-54a2-46fe-9c88-7bb3815b3397",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "A **tensor** is the fundamental data structure in TensorFlow. In the context of TensorFlow, a tensor is essentially a **multi-dimensional array** or **list of numbers**.\n",
    "\n",
    "It is the primary unit used to represent all data within a TensorFlow program, including inputs, outputs, model parameters, and intermediate results of calculations.\n",
    "\n",
    "### Tensor Structure and Analogy\n",
    "\n",
    "Tensors are simply generalizations of concepts you might already be familiar with:\n",
    "\n",
    "| Mathematical Name | Rank (Dimensions) | TensorFlow Analogy | Example |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Scalar** | 0-D | A single number | `7` |\n",
    "| **Vector** | 1-D | A list of numbers | `[1, 2, 3]` |\n",
    "| **Matrix** | 2-D | A table of numbers (rows and columns) | `[[1, 2], [3, 4]]` |\n",
    "| **3-Tensor** | 3-D | A cube or stack of matrices | `[[[...], [...]]]` |\n",
    "| **N-Tensor** | N-D | A data structure with N axes | Images (4-D), Video (5-D) |\n",
    "\n",
    "### Key Properties of a Tensor\n",
    "\n",
    "Every tensor in TensorFlow is defined by three main properties:\n",
    "\n",
    "1.  **Rank (or Axis):** The number of dimensions the tensor has. A rank-3 tensor has three axes.\n",
    "2.  **Shape:** The size of the tensor along each dimension. For example, a $2 \\times 3$ matrix has a shape of `(2, 3)`.\n",
    "3.  **Data Type (DType):** The type of data contained in the tensor's elements, such as `tf.float32`, `tf.int64`, or `tf.bool`.\n",
    "\n",
    "### Role in TensorFlow\n",
    "\n",
    "In TensorFlow, tensors serve as the **data carriers** that flow through the computational graph (the \"flow\" in TensorFlow). Every operation (such as addition, matrix multiplication, or ReLU activation) takes one or more tensors as input and produces one or more tensors as output.\n",
    "\n",
    "* **Input Data:** Images, text data, and numeric features are loaded and represented as tensors.\n",
    "* **Model Weights:** Trainable parameters (weights and biases) are stored as **`tf.Variable`** objects, which are special, mutable types of tensors.\n",
    "* **Calculations:** All mathematical operations performed during training and inference are tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519fae3-eef8-43d8-9be6-5f2086217c04",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada691f-0da7-4a1f-82d1-e9f4879c600a",
   "metadata": {},
   "source": [
    "Here are a few notes from [TensorFlow documentation](https://www.tensorflow.org/guide/autodiff):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564758d6-d098-4e82-95e1-eee4e36ea2de",
   "metadata": {},
   "source": [
    "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.\n",
    "\n",
    "In this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82beca0b-10d5-4207-a14d-2ba2b0481835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309ce76-b8fd-4000-8ba3-6d9d4f84c903",
   "metadata": {},
   "source": [
    "## Computing gradients\n",
    "\n",
    "To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the *forward pass*. Then, during the *backward pass*, TensorFlow traverses this list of operations in reverse order to compute gradients.\n",
    "\n",
    "\n",
    "## Gradient tapes\n",
    "\n",
    "TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variables`. TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation.\n",
    "\n",
    "Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec177cc-58a1-40bb-a4af-40075ed9aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape(persistent=False) as tape:\n",
    "    # y = x**2\n",
    "    y = tf.sin(x) + x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b623e-35e5-49fe-b3ec-87a15bd060ed",
   "metadata": {},
   "source": [
    "The `persistent=True` argument in `tf.GradientTape()` makes the tape \"persistent,\" meaning it will not discard the recorded operations after the first call to `tape.gradient()`. This allows you to calculate gradients multiple times using the same tape.\n",
    "\n",
    "By default, `tf.GradientTape()` is non-persistent (`persistent=False`). After you call `tape.gradient()` once, the computational resources used to store the intermediate forward-pass results are freed, and the tape is effectively destroyed. If you try to call `tape.gradient()` a second time on the same tape, you will get an error.\n",
    "\n",
    "Once you've recorded some operations, use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35662499-7966-4e5d-a813-5ad9ac06aee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(5.0100074)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c5ef6-da29-48dd-b581-3cfa69a104fe",
   "metadata": {},
   "source": [
    "The above example uses scalars, but `tf.GradientTape` works as easily on any tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a8ba020-7a3d-41c2-b4db-1ae1daf36fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3,2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = tf.matmul(x, w) + b\n",
    "    loss = tf.reduce_mean(y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b42d4b-13f1-4308-a07b-530fb89a7211",
   "metadata": {},
   "source": [
    "To get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see `tf.nest`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b70471c2-7f66-4b15-b1fd-769e9ff78573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 2.7180297,  4.258385 ],\n",
       "        [ 5.4360595,  8.51677  ],\n",
       "        [ 8.154089 , 12.775156 ]], dtype=float32)>,\n",
       " 'b': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.7180297, 4.258385 ], dtype=float32)>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grads = tape.gradient(loss, [w, b])\n",
    "grads = tape.gradient(loss, {'w': w, 'b': b})\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58532f36-7389-4f61-98d1-24863f65cf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.7180297,  4.258385 ],\n",
       "       [ 5.4360595,  8.51677  ],\n",
       "       [ 8.154089 , 12.775156 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['w'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b57dd-a481-42be-abd8-5d408b7445b5",
   "metadata": {},
   "source": [
    "## Gradients with respect to a model\n",
    "\n",
    "It's common to collect `tf.Variables` into a `tf.Module` or one of its subclasses (`layers.Layer, keras.Model`) for checkpointing and exporting.\n",
    "\n",
    "In most cases, you will want to calculate gradients with respect to a model's trainable variables. Since all subclasses of `tf.Module` aggregate their variables in the `Module.trainable_variables` property, you can calculate these gradients in a few lines of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "168d98c8-dc2f-4984-b332-ace187d8196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(units=2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients wrt every trainable variable\n",
    "grads = tape.gradient(loss, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0efa3721-1731-462c-b891-07f5ce797d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel, shape: (3, 2)\n",
      "[[ 0.2148267  -0.10884553]\n",
      " [-0.18049848  0.45471025]\n",
      " [ 0.55419815  0.7707133 ]]\n",
      "[[1.5164242 3.112715 ]\n",
      " [3.0328484 6.22543  ]\n",
      " [4.5492725 9.338145 ]]\n",
      "bias, shape: (2,)\n",
      "[0. 0.]\n",
      "[1.5164242 3.112715 ]\n"
     ]
    }
   ],
   "source": [
    "for var, grad in zip(layer.trainable_variables, grads):\n",
    "    print(f'{var.name}, shape: {grad.shape}')\n",
    "    print(var.numpy())\n",
    "    print(grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfca9ea-6819-4a1b-9c9c-e909dc65a469",
   "metadata": {},
   "source": [
    "## Controlling what the tape watches\n",
    "\n",
    "The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n",
    "\n",
    "* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
    "* The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
    "* The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
    "\n",
    "For example, the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af722d4f-c366-45b8-93ed-ba49119ee34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a variable: A variable + tensor returns a tensor\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2) + x3\n",
    "\n",
    "grads = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for grad in grads:\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e15600-2ad7-48b8-a837-0bc61bc55d43",
   "metadata": {},
   "source": [
    "You can list the variables being watched by the tape using the `GradientTape.watched_variables` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ea4ba4c-0d07-4af7-9cb8-670da323efd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tape.watched_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df707c38-f2f2-40ce-a8c5-1ba67a1d59d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0:0']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var.name for var in tape.watched_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7c188-9645-4485-8100-85cc8a974c2c",
   "metadata": {},
   "source": [
    "`tf.GradientTape` provides hooks that give the user control over what is or is not watched. To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ca5cf0a-a3bb-4579-a5fb-54ab15b81140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c998be3-10c3-4053-94bf-20633b10bfc6",
   "metadata": {},
   "source": [
    "Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99b3b57c-9fdd-4c1d-842e-763f086b1803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x0': None,\n",
       " 'x1': <tf.Tensor: shape=(), dtype=float32, numpy=0.9999545812606812>}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x1)\n",
    "    y0 = tf.math.sin(x0)\n",
    "    y1 = tf.nn.softplus(x1) # Softplus(x)=ln(1+ex)\n",
    "    y = y0 + y1                                              \n",
    "    ys = tf.reduce_sum(y)\n",
    "\n",
    "grads = tape.gradient(y, {'x0': x0, 'x1': x1})\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1f19f-d671-4fdc-923c-610163bfde05",
   "metadata": {},
   "source": [
    "## Intermediate results\n",
    "You can also request gradients of the output with respect to intermediate values computed inside the `tf.GradientTape` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bdfcfae-1264-413b-952c-2bcf0488cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "\n",
    "# Use the tape to compute the gradient of z with respect to the\n",
    "# intermediate value y.\n",
    "# dz_dy = 2 * y and y = x ** 2 = 9\n",
    "grad = tape.gradient(z, y)\n",
    "print(grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f672bb-e9d6-4f3a-91ea-4d0231001184",
   "metadata": {},
   "source": [
    "By default, the resources held by a GradientTape are released as soon as the `GradientTape.gradient` method is called. To compute multiple gradients over the same computation, create a gradient tape with `persistent=True`. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff7fda71-444e-45b2-a4ee-cc3426463a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "print(tape.gradient(z, x).numpy())  # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0])\n",
    "print(tape.gradient(y, x).numpy())  # [2.0, 6.0] (2 * x at x = [1.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1603909e-603a-450c-ae56-511a1ae76a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tape   # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c753be-371b-4210-9f29-710c83690217",
   "metadata": {},
   "source": [
    "## Notes on performance\n",
    "\n",
    "* There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n",
    "* Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n",
    "* For efficiency, some ops (like `ReLU`) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use `persistent=True` on your tape, nothing is discarded and your peak memory usage will be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e3061-87bb-4ad5-806c-feede0e9dcdc",
   "metadata": {},
   "source": [
    "## Gradients of non-scalar targets\n",
    "\n",
    "A gradient is fundamentally an operation on a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "535ba9fa-4e45-481f-9913-cfe2e5e7a220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient(y0, x).numpy())\n",
    "print(tape.gradient(y1, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baeee32-1289-41cb-944e-6ae9b3f7f4ac",
   "metadata": {},
   "source": [
    "Thus, if you ask for the gradient of multiple targets, the result for each source is:\n",
    "\n",
    "* The gradient of the sum of the targets, or equivalently\n",
    "* The sum of the gradients of each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8996335-8e17-4cc0-a115-0283e0e8a000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n"
     ]
    }
   ],
   "source": [
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "22f39b46-4fc7-456d-92aa-dc8611f25c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3de96-ac64-4c48-8a71-a44abaf6dbda",
   "metadata": {},
   "source": [
    "Similarly, if the target(s) are not scalar the gradient of the sum is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55589bb7-5c39-410c-aa87-688cd7bc7334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * [3., 4.]\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3c99b-bb86-4508-91c8-be7f6113c044",
   "metadata": {},
   "source": [
    "This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n",
    "\n",
    "If you need a separate gradient for each item, refer to `Jacobians`.\n",
    "\n",
    "In some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "159b18d6-ec55-48ab-b727-c458d65293a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-10.0, 10.0, 200)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.nn.sigmoid(x)\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "# print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e2a104cd-ed7d-43ca-ba86-2e68bbf382bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7a7928d01e50>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, 'r-');\n",
    "plt.plot(x, grad, 'b-');\n",
    "plt.legend(['y', 'dy/dx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea3155-0e2c-4726-9d3d-072b918607a7",
   "metadata": {},
   "source": [
    "## Control flow\n",
    "\n",
    "Because a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, `if` and `while` statements).\n",
    "\n",
    "Here a different variable is used on each branch of an `if`. The gradient only connects to the variable that was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6dd004e7-1cc6-4c15-aabd-488c2a1fc322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.0)\n",
    "\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  if x > 0.0:\n",
    "    result = v0\n",
    "  else:\n",
    "    result = v1**2 \n",
    "\n",
    "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
    "\n",
    "print(dv0)\n",
    "print(dv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba35443-2dcf-4f01-bc8e-46ac78725edd",
   "metadata": {},
   "source": [
    "Just remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n",
    "\n",
    "Depending on the value of `x` in the above example, the tape either records `result = v0` or `result = v1**2`. The gradient with respect to `x` is always `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b261a496-38e2-43e3-b9a6-f3349b5ef3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "dx = tape.gradient(result, x)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52d46d-b3f1-4885-a3d7-b7a83108f23b",
   "metadata": {},
   "source": [
    "## Cases where gradient returns None\n",
    "\n",
    "When a target is not connected to a source, gradient will return `None`:\n",
    "1. Replaced a variable with a tensor\n",
    "2. Did calculations outside of TensorFlow\n",
    "3. Took gradients through an integer or string\n",
    "4. Took gradients through a stateful object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530244c-3931-454b-a8b6-ee16e7f65c57",
   "metadata": {},
   "source": [
    "### 1. Replaced a variable with a tensor\n",
    "\n",
    "In the section on \"controlling what the tape watches\" you saw that the tape will automatically watch a `tf.Variable` but not a `tf.Tensor`.\n",
    "\n",
    "One common error is to inadvertently replace a `tf.Variable` with a `tf.Tensor`, instead of using `Variable.assign` to update the `tf.Variable`. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f0098a7d-5ea7-4377-9126-66fc457ce262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "3.0\n",
      "ResourceVariable : tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x**2+1\n",
    "    \n",
    "    print(type(x).__name__, \":\", tape.gradient(y, x))\n",
    "    # x = x + 1   # This should be `x.assign_add(1)`\n",
    "    x.assign_add(1.0)\n",
    "    # x.assign(x*2.0)\n",
    "    print(x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd67906-ba2f-42a9-afa6-d8c5d8fab2a4",
   "metadata": {},
   "source": [
    "### 2. Did calculations outside of TensorFlow\n",
    "\n",
    "The tape can't record the gradient path if the calculation exits TensorFlow. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "afb0322c-0759-4455-89fa-a1aa28e3a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5. 10.]\n",
      "None\n",
      "tf.Tensor(7.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x2 = x**2\n",
    "        \n",
    "    # This step is calculated with NumPy\n",
    "    y = np.mean(x2, axis=0)\n",
    "    print(y)\n",
    "    # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "    # using `tf.convert_to_tensor`.\n",
    "    y = tf.reduce_mean(y, axis=0)\n",
    "\n",
    "print(tape.gradient(y, x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4016c5f-9412-4fb0-9264-54ea51a72df3",
   "metadata": {},
   "source": [
    "### 3. Took gradients through an integer or string\n",
    "\n",
    "Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient. Nobody expects strings to be differentiable, but it's easy to accidentally create an int constant or variable if you don't specify the dtype.\n",
    "\n",
    "TensorFlow doesn't automatically cast between types, so, in practice, you'll often get a type error instead of a missing gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ff6d6165-4499-4f61-97c7-362fecd013e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "\n",
    "print(g.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d66d04-f4f1-401e-9cd2-3287658b6a1d",
   "metadata": {},
   "source": [
    "### 4. Took gradients through a stateful object\n",
    "\n",
    "State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n",
    "\n",
    "A `tf.Tensor` is immutable. You can't change a tensor once it's created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a `tf.matmul` only depends on its inputs.\n",
    "\n",
    "A `tf.Variable` has internal state  its value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "686fde05-3b7f-44eb-b50b-c399ebce5cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Update x1 = x1 + x0.\n",
    "    x1.assign_add(x0)\n",
    "    # The tape starts recording from x1.\n",
    "    y = x1**2   # y = (x1 + x0)**2\n",
    "\n",
    "# This doesn't work.\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5522c-f1f0-4f3a-b12b-583df1f5138a",
   "metadata": {},
   "source": [
    "Similarly, tf.data.Dataset iterators and tf.queues are stateful, and will stop all gradients on tensors that pass through them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea7364-c816-4ca5-b661-32e1423fd0fe",
   "metadata": {},
   "source": [
    "## Zeros instead of None\n",
    "\n",
    "In some cases it would be convenient to get 0 instead of `None` for unconnected gradients. You can decide what to return when you have unconnected gradients using the `unconnected_gradients` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b862916c-c967-4406-8abc-24adb7dfd0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = y**2\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454eb8c-3a56-4da9-87fd-722e446b348c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
