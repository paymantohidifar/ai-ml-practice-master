{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ea01ce-bf13-4f0f-bcd3-9850f988ec2d",
   "metadata": {},
   "source": [
    "# Week 1: ML Strategy\n",
    "\n",
    "Streamline and optimize your ML production workflow by implementing strategic guidelines for goal-setting and applying human-level performance to help define key priorities.\n",
    "Learning Objectives\n",
    "\n",
    "* Explain why Machine Learning strategy is important\n",
    "* Apply satisficing and optimizing metrics to set up your goal for ML projects\n",
    "* Choose a correct train/dev/test split of your dataset\n",
    "* Define human-level performance\n",
    "* Use human-level performance to define key priorities in ML projects\n",
    "* Take the correct ML Strategic decision based on observations of performances and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73457187",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4d379",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a6d79",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73666143-155c-43c3-b73c-55def1c64551",
   "metadata": {},
   "source": [
    "## Orthogonalization in ML\n",
    "\n",
    "This section introduces the concept of **orthogonalization** as a strategy for developing effective machine learning systems, advocating for clarity on *what* to tune to achieve a single, specific effect.\n",
    "\n",
    "### Concept of Orthogonalization\n",
    "\n",
    "Orthogonalization means designing controls (or hyperparameters) so that each control ideally affects only one specific dimension of the system's performance.\n",
    "\n",
    "For example, a well-designed TV has separate knobs for height, width, rotation, etc. A non-orthogonal knob would change all these aspects simultaneously, making precise tuning nearly impossible.\n",
    "\n",
    "Having orthogonal controls makes the process of tuning and debugging much easier because you know exactly which knob to turn when a specific problem is identified.\n",
    "\n",
    "### The Four Performance Criteria\n",
    "\n",
    "For a supervised learning system to perform well, it generally needs to achieve four goals sequentially. Orthogonalization helps by providing a distinct \"knob\" for addressing each potential failure point:\n",
    "\n",
    "#### 1.  Fit the Training Set Well (Low Bias)\n",
    "\n",
    "* **Problem:** The algorithm is not performing well on the training data.\n",
    "* **Orthogonal Knobs:** Use a bigger network (more capacity) or switch to a better optimization algorithm (e.g., Adam).\n",
    "\n",
    "#### 2. Fit the Development Set Well (Low Variance)\n",
    "\n",
    "* **Problem:** The algorithm fits the training set well but performs poorly on the dev set (overfitting).\n",
    "* **Orthogonal Knobs:** Apply regularization (to reduce variance) or get a bigger training set (to improve generalization).\n",
    "\n",
    "#### 3. Fit the Test Set Well\n",
    "\n",
    "* **Problem:** Performance is good on the dev set but poor on the test set.\n",
    "* **Orthogonal Knob:** Get a bigger dev set. This indicates the dev set itself was too small, leading to overfitting the dev set metrics.\n",
    "\n",
    "#### 4. Perform Well in the Real World\n",
    "\n",
    "* **Problem:** Performance is good on the test set metric, but the system doesn't deliver the desired real-world value.\n",
    "* **Orthogonal Knob:** Change the Dev/Test set distribution or change the cost function because the current metric is not accurately measuring the real-world goal.\n",
    "\n",
    "### Example of Non-Orthogonal Control\n",
    "\n",
    "**Early Stopping:** This technique is considered less orthogonal because it simultaneously affects two criteria:\n",
    "1.  It improves dev set performance (reducing variance).\n",
    "2.  It reduces training set performance (increasing bias/not fitting training data as well).\n",
    "\n",
    "While it is not a bad technique, using more orthogonal controls makes the system easier to reason about and tune.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The goal is to diagnose the exact bottleneck in performance (which of the four criteria is failing) and then use a specific, orthogonal set of controls to fix only that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c56650",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c359a1b-594e-4217-b3da-9bd37df8b595",
   "metadata": {},
   "source": [
    "## Single Number Evaluation Metric\n",
    "\n",
    "This section emphasizes the critical role of a **single, real-number evaluation metric** in accelerating the iterative process of developing and improving machine learning algorithms.\n",
    "\n",
    "### The Need for a Single Metric\n",
    "\n",
    "A single real-number evaluation metric allows a team to quickly and definitively determine whether a new idea, hyperparameter change, or algorithm modification is better or worse than the previous one.\n",
    "* **Empirical Process:** Machine learning development is highly empirical (Idea $\\rightarrow$ Code $\\rightarrow$ Experiment $\\rightarrow$ Refine). A single metric speeds up this iterative loop.\n",
    "\n",
    "### Dealing with Multiple Criteria\n",
    "\n",
    "When using multiple metrics (like Precision and Recall), it becomes difficult to choose between competing classifiers when none of them dominates all metrics.\n",
    "\n",
    "#### Example 1: Precision and Recall\n",
    "\n",
    "* **Metrics:**\n",
    "    * **Precision (P):** Of the examples identified as positive (e.g., cats), what percentage are correct?\n",
    "    * **Recall (R):** Of all the true positive examples (actual cats), what percentage were correctly identified?\n",
    "* **Solution:** Combine multiple metrics into a single number. The standard way to combine Precision and Recall is the **F1 Score**, which is Harmonic Mean of $P$ and $R$ (This averages P and R while favoring high values for both):\n",
    "\n",
    "$$F1 = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}}$$\n",
    "\n",
    "#### Example 2: Error Across Geographies\n",
    "\n",
    "* **Metrics:** Tracking error rates across multiple markets (e.g., US, China, India, Other).\n",
    "* **Solution:** Compute a simple average of the error rates across all markets. This provides a single number to compare algorithms quickly, assuming average performance is a reasonable proxy for overall success.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "A well-defined development (dev) set combined with a single, real-number evaluation metric is essential for efficiently comparing and selecting the best algorithm during the development process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84e1df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f685cb-aeb1-4d6e-832a-fa938e91561e",
   "metadata": {},
   "source": [
    "## Satisficing and Optimizing Metric\n",
    "\n",
    "This section introduces the concepts of **Optimizing** and **Satisficing** metrics as a practical strategy for evaluating and selecting machine learning models when multiple factors are important and difficult to combine into a single formula.\n",
    "\n",
    "### When to Use\n",
    "\n",
    "This approach is useful when you care about multiple performance metrics (e.g., accuracy, speed, cost) but find it difficult or arbitrary to combine them mathematically (e.g., using a weighted sum). If you have $N$ metrics you care about, it's often practical to:\n",
    "* Choose one metric to be optimizing.\n",
    "* Choose the remaining $N-1$ metrics to be satisficing.\n",
    "\n",
    "### Optimizing Metric\n",
    "\n",
    "The single metric you wish to maximize (or minimize, depending on the goal). You aim to achieve the best possible performance on this metric.\n",
    "\n",
    "**Example (Cat Classifier):** Accuracy (or F1 Score).\n",
    "\n",
    "### Satisficing Metric\n",
    "\n",
    "One or more metrics that only need to reach a **\"good enough\" threshold**. Once the threshold is met, further improvement on this metric is not prioritized.\n",
    "\n",
    "**Example (Cat Classifier):** Running Time must be $\\le 100$ milliseconds.\n",
    "* In a set of classifiers, you first filter out any that fail the satisficing criteria.\n",
    "* You then select the classifier from the remaining set that has the highest value for the optimizing metric. (E.g., Classifier B maximizes accuracy while still meeting the running time requirement).\n",
    "\n",
    "### Summary\n",
    "\n",
    "By defining one optimizing metric and one or more satisficing metrics, you create a clear, almost automatic decision rule for quickly selecting the \"best\" algorithm among many choices, thus speeding up the development iteration cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b677d39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e8f6c-51f1-421c-b4cf-c5349aff4f32",
   "metadata": {},
   "source": [
    "## Train/Dev/Test Distributions\n",
    "\n",
    "This section provides crucial advice on setting up development (dev) and test sets to maximize the efficiency and effectiveness of machine learning development, primarily by ensuring they share the same data distribution.\n",
    "\n",
    "### Importance of Setup\n",
    "\n",
    "The way you define your training, development (dev), and test sets significantly impacts how quickly your team can make progress. The *dev set + single real-number evaluation metric* defines the *target* that the team aims for.\n",
    "\n",
    "### Bad Practice: Different Distributions\n",
    "\n",
    "If the dev and test sets come from different distributions, the team might spend months optimizing performance for the dev set, only to find the model performs poorly on the test set. This is akin to training for one target and then suddenly being asked to hit a different, unexpected target.\n",
    "\n",
    "### Best Practice: Same Distribution\n",
    "\n",
    "The dev set and the test set should come from the same distribution. For example:\n",
    "\n",
    "* Take all available data (e.g., data from all eight geographic regions, or all income levels) and **randomly shuffle** it before splitting it into dev and test sets.\n",
    "* Ensure the dev and test sets reflect the same type of data you expect to encounter and want to perform well on in the future.\n",
    "\n",
    "### Real-World Pitfall Example\n",
    "\n",
    "A team spent months optimizing a loan approval model using a dev set composed of data from *medium-income postal codes*. They later tested the model on data from *low-income postal codes* (a different distribution), and the model failed completely, wasting three months of work.\n",
    "\n",
    "### Summary\n",
    "\n",
    "* The dev set and the evaluation metric define the target your team works to hit.\n",
    "* By ensuring the dev and test sets are drawn from the same, representative data distribution, you ensure that optimizing performance on the dev set directly translates to good, expected performance on the test set, leading to much more efficient iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef310d7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb2e25-1bf0-49a4-94e8-8e508fc6109c",
   "metadata": {},
   "source": [
    "### Size of dev and test sets\n",
    "\n",
    "This section discusses how the guidelines for splitting data into training, development (dev), and test sets have changed in the deep learning (big data) era, shifting away from traditional percentage splits.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Bullet Points: Sizing Dev and Test Sets\n",
    "\n",
    "##### Shift in Data Splitting Rules\n",
    "* **Old Rule of Thumb:** In earlier eras with smaller datasets (hundreds to tens of thousands of examples), the common split was **70% Train / 30% Test** or **60% Train / 20% Dev / 20% Test**.\n",
    "* **Modern Deep Learning Trend:** Due to the large size of modern datasets (e.g., millions of examples) and the high data hunger of deep learning algorithms, the trend is to allocate a much **larger fraction to the training set** and a much **smaller fraction to the dev/test sets**.\n",
    "\n",
    "##### Guidelines for Modern Splitting\n",
    "1.  **Training Set (Largest Fraction):** The training set should consume the largest fraction of the data, potentially **98% or more** when dealing with millions of examples.\n",
    "2.  **Dev Set (Sufficient for Evaluation):** The dev set's purpose is to evaluate different ideas and choose the best algorithm. It needs to be big enough to give confidence in rank-ordering different models (e.g., 1% of 1 million examples, or 10,000 examples, might be sufficient).\n",
    "3.  **Test Set (Sufficient for Final Confidence):** The test set's purpose is to provide an unbiased evaluation of the final system before deployment.\n",
    "    * It must be **big enough to give high confidence** in the system's overall performance.\n",
    "    * Similar to the dev set, this size may be far **less than 30%** of the total data (e.g., 1% or 10,000 examples).\n",
    "\n",
    "##### Train/Dev/Test Set vs. Train/Dev Set\n",
    "* **Ideal Practice:** It is reassuring to maintain a separate test set to get an unbiased estimate of the final system's performance.\n",
    "* **Train/Dev Only:** In some situations, a team might only use a Train/Dev split and not worry about a separate test set, especially if the dev set is very large. However, this is unusual and not generally recommended, as it means the team is optimizing directly to their final evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec70d0c-c2fc-44b2-b44d-b46c95c1066c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### When to change to Dev/Test Sets and Metrics\n",
    "\n",
    "This section explains that the chosen evaluation metric and development (dev) set are like a **target** for a machine learning team, and if they stop correctly rank-ordering algorithms based on real-world preferences, they must be changed.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Bullet Points: Moving the Target (Changing Metrics and Data)\n",
    "\n",
    "##### When to Change the Target\n",
    "The core guideline is that if doing well on your current metric and dev/test set does not correspond to doing well on the application you actually care about, change the metric and/or the data.\n",
    "\n",
    "1.  **Metric Misranks Algorithms (Unacceptable Errors):**\n",
    "    * **Problem:** An algorithm (A) achieves better performance on the simple classification error metric (e.g., 3% error) but is fundamentally worse because it fails a crucial constraint (e.g., letting through unacceptable content like pornography).\n",
    "    * **Solution:** Change the Evaluation Metric - Introduce **weighted misclassification error** where unacceptable errors (e.g., mislabeling a pornographic image as a cat) are penalized much more heavily (e.g., a weight of 10x or 100x).\n",
    "      $$\\text{Error} = \\frac{1}{\\sum_iw^{(i)}}\\sum_i w^{(i)}I(\\hat y^{(i)}, y^{(i)})$$\n",
    "\n",
    "      where $w^{(i)}$ is defined as below:\n",
    "      $$w^{(i)} = \\begin{cases} 1 & \\text{if } x^{(i)} \\text{ is non-porn} \\\\ 10 & \\text{if } x^{(i)} \\text{ is porn} \\end{cases}$$\n",
    "      \n",
    "2.  **Dev/Test Set Does Not Reflect Reality (Data Mismatch):**\n",
    "    * **Problem:** The dev/test set contains high-quality, well-framed images (e.g., downloaded from the internet), but the deployed application uses low-quality, blurry, or poorly framed user-uploaded images. An algorithm that performs better in the real app might perform worse on the high-quality dev set.\n",
    "    * **Solution:** Change the Dev/Test Set Distribution - Update your evaluation data to better reflect the true distribution of data the algorithm will encounter in production (e.g., include blurrier, less professional photos from the mobile app).\n",
    "\n",
    "##### Orthogonalization Principle\n",
    "* **Target Setting (Step 1):** Defining the metric is a distinct step from achieving good performance on that metric.\n",
    "* **Aiming and Shooting (Step 2):** After defining the metric, the separate step is figuring out how to do well on it (e.g., changing the neural network's internal cost function, $J$, to align with the new weighted external metric).\n",
    "\n",
    "#### Final Guidance\n",
    "* **Start Quickly:** Don't wait for the \"perfect\" metric or dev set; quickly set something up to start the iteration process. Running without any metric and dev set slows down team efficiency.\n",
    "* **Be Flexible:** It is perfectly acceptable to change the metric and/or data later if you discover a better approach that more accurately captures your application's true performance requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7497f9b-57ca-489a-b72d-e510b518bdfe",
   "metadata": {},
   "source": [
    "### Why human-level performance?\n",
    "\n",
    "This section explains that **Human-Level Performance (HLP)** serves as a crucial estimate for the **Bayes Optimal Error** (the theoretically lowest possible error rate) and is essential for properly diagnosing whether a model's poor performance is due to **high bias** or **high variance**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Bullet Points: Human-Level Performance and Error Analysis\n",
    "\n",
    "##### 1. Human-Level Error as a Proxy for Bayes Error\n",
    "* **Bayes Optimal Error (Bayes Error):** The lowest possible theoretical error rate for any classifier on a given dataset. You cannot perform better than this without overfitting.\n",
    "* **Human-Level Performance (HLP):** For tasks humans are very good at (like computer vision), HLP is a reasonable, practical proxy or estimate for the Bayes Error.\n",
    "* **Goal:** The ultimate goal is to reach or closely approach the Bayes Error. You should avoid reducing training error below the Bayes Error, as this likely leads to overfitting.\n",
    "\n",
    "##### 2. Decomposing Error: Avoidable Bias vs. Variance\n",
    "The comparison between HLP, Training Error, and Dev Error determines the focus of improvement:\n",
    "\n",
    "| Error Component | Calculation | Problem Type | Tactics to Reduce |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Avoidable Bias** | $\\text{Training Error} - \\text{HLP (Bayes Error estimate)}$ | High Bias (Underfitting) | Train a bigger network, run longer training, try a better optimization algorithm. |\n",
    "| **Variance** | $\\text{Dev Error} - \\text{Training Error}$ | High Variance (Overfitting) | Apply regularization**, get more training data. |\n",
    "\n",
    "##### 3. Impact of HLP on Strategy (Example Comparison)\n",
    "\n",
    "| Scenario | HLP (Bayes Error Est.) | Training Error | Dev Error | Avoidable Bias | Variance | Focus |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **Case 1** | **1.0%** | 8.0% | 10.0% | **7.0%** | 2.0% | **Reduce Bias** (7.0% gap is large) |\n",
    "| **Case 2** | **7.5%** | 8.0% | 10.0% | **0.5%** | 2.0% | **Reduce Variance** (2.0% gap is larger) |\n",
    "\n",
    "In Case 2, reducing the training error below 7.5% offers little theoretical benefit and risks overfitting, so efforts shift to reducing the variance gap.\n",
    "\n",
    "#### Accuracy Improvement Over Time\n",
    "\n",
    "![Human Level Performance](images/hlp.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
