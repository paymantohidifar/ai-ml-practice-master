{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f4f43a-f551-4e04-9429-4d061828894c",
   "metadata": {},
   "source": [
    "# Week 2: ML Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e37eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597da10",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524216e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad00fc8",
   "metadata": {},
   "source": [
    "## Carrying out Error Analysis\n",
    "\n",
    "This section explains the process of **Error Analysis**, a crucial manual diagnostic procedure used in machine learning to systematically prioritize which types of mistakes are most worthwhile to fix.\n",
    "\n",
    "### Purpose of Error Analysis\n",
    "\n",
    "* Error Anlaysis is used when a learning algorithm's performance is below the desired level (often human-level performance).\n",
    "* The goal is to quickly estimate the **ceiling on performance** (maximum potential improvement) for fixing a specific type of error, thereby helping to prioritize development effort.\n",
    "* The methodology is to manually examining a sample of mislabeled or misclassified examples from the development (dev) set.\n",
    "\n",
    "### The Simple Counting Procedure\n",
    "\n",
    "1.  **Collect Sample:** Get a sample of mislabeled dev set examples (e.g., 100 examples).\n",
    "2.  **Manual Inspection:** Manually examine each mislabeled example.\n",
    "3.  **Count and Estimate:** Count how many errors fall into a specific category (e.g., dogs misclassified as cats).\n",
    "4.  **Calculate Ceiling:** Estimate the maximum possible improvement in accuracy if that specific error category were completely solved.\n",
    "\n",
    "* **Example:** If the current error rate is $10\\%$ and $5\\%$ of the errors are due to dogs:\n",
    "    * Maximum improvement is $5\\%$ of the total error.\n",
    "    * New error ceiling: $10\\% - (10\\% \\times 0.05) = 9.5\\%$ error. (A small relative gain.)\n",
    "* **Example (High Potential):** If $50\\%$ of the errors are due to dogs:\n",
    "    * New error ceiling: $10\\% - (10\\% \\times 0.50) = 5\\%$ error. (A huge relative gain, worth significant effort.)\n",
    "\n",
    "### Error Analysis with Multiple Categories\n",
    "\n",
    "When considering multiple ideas for improvement, a structured table or spreadsheet is recommended:\n",
    "\n",
    "| Idea/Category | Description | Data Collection |\n",
    "| :--- | :--- | :--- |\n",
    "| **Setup:** | List misclassified images (e.g., 1 to 100). | Create columns for each error idea (e.g., Dogs, Great Cats, Blurry Images). |\n",
    "| **Execution:** | For each image, place a checkmark in the relevant column(s). | Use a comments section to briefly describe the mistake (e.g., \"Pit bull picture,\" \"Lion, rainy day at zoo\"). |\n",
    "| **Prioritization:** | Calculate the percentage of total errors belonging to each category. | Focus effort on the categories that account for the largest fraction of the errors (highest performance ceiling). |\n",
    "| **Adaptability:** | New error categories can be added on the fly during the manual inspection process (e.g., \"Instagram Filters\"). | This allows the analysis to be guided by what the data is actually showing. |\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Error analysis is a fast, low-effort procedure (often 5-10 minutes for 100 examples) that provides crucial data for making strategic, high-impact decisions, helping developers avoid spending months of work on problems with a low performance ceiling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338d509",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd332c2",
   "metadata": {},
   "source": [
    "## Cleaning Up Incorrectly Labeled Data\n",
    "\n",
    "This section addresses the issue of **incorrectly labeled examples** (errors in the true $Y$ values) in a dataset and provides guidelines on whether and how to fix them, particularly in the context of error analysis.\n",
    "\n",
    "### Training Set Errors\n",
    "\n",
    "* **Robustness to Random Errors:** Deep learning algorithms are generally robust to random errors in the training set labels, provided the total dataset size is large and the percentage of errors is not too high. It's often acceptable to leave minor random errors as they are.\n",
    "* **Vulnerability to Systematic Errors:** Deep learning algorithms are less robust to systematic errors (e.g., if the labeler consistently labels all white dogs as cats). Systematic errors must be addressed as they introduce damaging bias.\n",
    "* **Training vs. Dev/Test:** It is less critical to fix labels in the training set than in the dev/test sets, which are used for crucial evaluation.\n",
    "\n",
    "### Dev/Test Set Errors (The Priority)\n",
    "\n",
    "The dev and test sets are used to evaluate models and choose between them, making label accuracy here more important.\n",
    "\n",
    "* **Error Analysis Integration:** During the manual error analysis process, an extra column should be added to count the percentage of mistakes where the classifier disagreed with the label simply because the label itself was incorrect in the dev set.\n",
    "* **Decision Criterion:** Fix incorrect labels in the dev/test sets only if they make a significant difference to the ability to accurately evaluate and compare classifiers.\n",
    "\n",
    "| Scenario | Overall Dev Error | Error Due to Incorrect Labels | Error Due to Algorithm ($9.4\\%$) | Conclusion |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Case 1 (Low Impact)** | $10\\%$ | $0.6\\%$ ($6\\%$ of total error) | $9.4\\%$ | **Low Priority:** The $0.6\\%$ error is a small fraction of the total $10\\%$ error. Focus on the larger $9.4\\%$ algorithmic error. |\n",
    "| **Case 2 (High Impact)** | $2\\%$ | $0.6\\%$ ($30\\%$ of total error) | $1.4\\%$ | **High Priority:** The $0.6\\%$ error is now a large fraction ($30\\%$) of the total $2\\%$ error. This noise makes it difficult to reliably compare two high-performing classifiers (e.g., $2.1\\%$ vs $1.9\\%$ error). **Fix labels first.** |\n",
    "\n",
    "### Guidelines for Fixing Labels\n",
    "\n",
    "If you decide to manually fix labels in the dev/test sets, follow these principles:\n",
    "\n",
    "1.  **Apply to Both:** Apply the label correction process consistently to both the dev and test sets to ensure they maintain the same data distribution.\n",
    "2.  **Examine Correct and Incorrect Examples (Ideal):** Ideally, you should examine the labels for both examples the algorithm got wrong and examples it got right.\n",
    "    * *Reality Check:* This is often impractical if the algorithm is highly accurate ($98\\%$ correct), as it would require checking $98\\%$ of the data. Often, teams only check labels for examples the classifier got wrong.\n",
    "3.  **Training Set Optional:** You can choose to fix labels only in the smaller dev/test sets and leave the errors in the much larger training set. This is acceptable, although it introduces a slight distribution difference between the training and dev/test sets.\n",
    "\n",
    "### Importance of Human Insight\n",
    "\n",
    "* **Beyond Automaton:** While deep learning emphasizes feeding data to an algorithm, building practical systems still requires manual error analysis and human insight.\n",
    "* **Prioritization Tool:** Spending a short amount of time (minutes or a few hours) manually examining data to count error categories is an extremely efficient way to prioritize development directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfa2eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caacd63",
   "metadata": {},
   "source": [
    "## Build your First System Quickly, then Iterate\n",
    "\n",
    "This section advises adopting an **iterative, build-it-quick** approach when starting a brand new machine learning application to avoid prematurely prioritizing the wrong technical direction.\n",
    "\n",
    "### The Challenge of Prioritization\n",
    "\n",
    "For any new machine learning application (e.g., speech recognition), there are dozens of plausible areas for improvement (e.g., noisy backgrounds, accented speech, far-field speakers, children's speech, output fluency).\n",
    "\n",
    "Even experts find it difficult to prioritize which direction to focus on without first analyzing the problem's specific characteristics and current system performance.\n",
    "\n",
    "### The Recommended Iterative Strategy\n",
    "\n",
    "The recommended approach for starting a new machine learning application is to \"Build the first system quickly, then iterate.\"\n",
    "\n",
    "1.  **Quick Setup (The Target):** Quickly set up your dev/test sets and define the core evaluation metric (It's okay if this target needs to be adjusted later).\n",
    "2.  **Quick Build (The Baseline):** Build an initial, functional machine learning system quickly. This should be a \"quick and dirty\" implementation â€” don't overcomplicate it.\n",
    "3.  **Diagnosis and Prioritization:** Use the trained initial system to diagnose performance:\n",
    "    * **Bias/Variance Analysis:** Determine if the problem is primarily due to avoidable bias (underfitting) or variance (overfitting).\n",
    "    * **Error Analysis:** Manually examine the mistakes the system is making to quantify and prioritize the most frequent error categories (e.g., how many errors are due to \"far-field speech\").\n",
    "4.  **Iterate:** Use the results of the diagnosis (e.g., high error rate on far-field speech) to rationally prioritize the next development step.\n",
    "\n",
    "### The Value of the Initial System\n",
    "\n",
    "The main value of the initial quick-and-dirty system is that it allows the team to localize the problems through analysis:\n",
    "\n",
    "* It moves the team from guessing what the biggest problem is to knowing the biggest problem is (e.g., $50\\%$ of errors are far-field).\n",
    "* It provides the necessary data to apply strategic tools like Bias/Variance analysis and Error Analysis.\n",
    "\n",
    "### When This Advice Applies\n",
    "\n",
    "* **Strongly Applies:** When tackling a brand new application area where the team lacks significant prior experience.\n",
    "* **Less Strongly Applies:** When working in an application area with significant prior experience or a large body of academic literature (e.g., face recognition) that provides a clear starting architecture.\n",
    "\n",
    "### Common Pitfall\n",
    "\n",
    "Often teams do wrong by overthinking and building a system that is too complicated at the start, wasting valuable time before even knowing if they are prioritizing the right issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4437e0de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651349f6",
   "metadata": {},
   "source": [
    "## Training and Testing on Different Distributions\n",
    "\n",
    "This section addresses the common practice in the deep learning era of training models on data from a different distribution than the target distribution (dev/test sets) to maximize the training size.\n",
    "\n",
    "### The Deep Learning Data Dilemma\n",
    "\n",
    "Deep learning algorithms perform best with large amounts of labeled training data. Often, the available labeled data comes from two sources:\n",
    "1.  A small amount of data from the target distribution (the data you actually care about).\n",
    "2.  A large amount of data from a different, easily accessible distribution (e.g., crawled web images).\n",
    "\n",
    "### Strategy for Setting Up Train/Dev/Test Sets\n",
    "\n",
    "The critical rule is that the Dev and Test sets must come from the target distribution to ensure the team is optimizing performance where it matters most.\n",
    "\n",
    "* **Scenario:** Building a cat classifier for a mobile app (target distribution: blurry cell phone photos) using supplemental web-crawled data (different distribution: professional high-res photos).\n",
    "\n",
    "| Splitting Option | Dev/Test Distribution | Outcome | Recommendation |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Option 1 (Random Shuffle)** | Mixed (mostly web photos) | Sets the team's target on optimizing for the *web image* distribution, which is not the product's goal. | **Avoid this option.** |\n",
    "| **Option 2 (Targeted Split)** | All Mobile App photos | Aims the target correctly, forcing the algorithm to generalize to the data that matters (mobile app photos). | **Recommended strategy.** |\n",
    "\n",
    "* **Recommended Data Split Example (Option 2):**\n",
    "    * **Training Set:** Large, combined dataset (e.g., 200k web images + 5k mobile app images).\n",
    "    * **Dev Set:** Small dataset (e.g., 2.5k) entirely from the mobile app distribution.\n",
    "    * **Test Set:** Small dataset (e.g., 2.5k) entirely from the mobile app distribution.\n",
    "\n",
    "### Implications of Different Distributions\n",
    "\n",
    "* **Advantage:** Allows the team to utilize a much larger training set, leading to better overall performance.\n",
    "* **Disadvantage:** The training distribution now differs from the dev/test distributions. This introduces the new challenge of **Data Mismatch**, which requires specialized analysis techniques (to be discussed later).\n",
    "\n",
    "### General Application\n",
    "\n",
    "This strategy applies broadly whenever the data available for training differs from the data on which the final product will be evaluated (e.g., training a car speech system using general purpose speech data accumulated over years)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8966c57",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f560da2",
   "metadata": {},
   "source": [
    "## Bias and Variance with Mismatched Data Distributions\n",
    "\n",
    "This section explains how the analysis of bias and variance must be modified when the training data distribution differs from the dev/test data distribution, introducing the new diagnostic tool: the **Training-Dev Set**.\n",
    "\n",
    "### The Challenge of Data Mismatch\n",
    "\n",
    "When the training set distribution differs from the dev/test set distribution, a large gap between training error and dev error can no longer be definitively attributed to variance alone. The increase in error could be due to:\n",
    "1.  **Variance:** The algorithm failed to generalize to unseen data from the *same* distribution.\n",
    "2.  **Data Mismatch:** The dev set distribution is inherently *harder* or different than the training set distribution.\n",
    "\n",
    "### Introducing the Training-Dev Set\n",
    "\n",
    "To isolate the effects of variance and data mismatch, a new dataset subset is required:\n",
    "\n",
    "* **Training-Dev Set:** A subset of data randomly carved out from the Training Set distribution (the large source data).\n",
    "* **Purpose:** The model is trained only on the Training Set proper, not the Training-Dev set. The Training-Dev error is measured on data that is unseen but from the same distribution as the training data.\n",
    "\n",
    "### Diagnostic Gaps in Data Mismatch Setting\n",
    "\n",
    "By measuring error across four points (HLE, Training, Training-Dev, Dev), three distinct problems can be diagnosed:\n",
    "\n",
    "| Gap | Calculation | Problem | Focus for Improvement |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Gap 1** | HLE - Training Error | Avoidable Bias | Focus on fitting the training data better (e.g., bigger model, better optimization). |\n",
    "| **Gap 2** | Training-Dev Error - Training Error | Variance | Focus on generalization (e.g., regularization, more data from the training distribution). |\n",
    "| **Gap 3** | Dev Error - Training-Dev Error | Data Mismatch | Focus on making the model robust to differences between the source data and the target data. |\n",
    "| **Gap 4** | Test Error - Dev Error | Overfitting to Dev Set | Indicates the team over-optimized the dev set. Fix: Get a larger dev set. |\n",
    "\n",
    "### Example Scenarios\n",
    "\n",
    "| Training Error | Training-Dev Error | Dev Error | Diagnosis | Primary Focus |\n",
    "| :---: | :---: | :---: | :--- | :--- |\n",
    "| $1\\%$ | $9\\%$ | $10\\%$ | **High Variance** (Large Gap 2) | Reduce variance (regularization). |\n",
    "| $1\\%$ | $1.5\\%$ | $10\\%$ | **High Data Mismatch** (Large Gap 3) | Address data mismatch (techniques for distribution shift). |\n",
    "| $10\\%$ | $11\\%$ | $12\\%$ | **High Avoidable Bias** (Large Gap 1, assuming HLE $\\approx 0\\%$) | Reduce bias (bigger model). |\n",
    "| $10\\%$ | $11\\%$ | $20\\%$ | High Bias + **High Data Mismatch** (Large Gaps 1 & 3) | Address bias and data mismatch. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd0e08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed0613",
   "metadata": {},
   "source": [
    "## Addressing Data Mismatch\n",
    "\n",
    "This section focuses on the practical steps and inherent risks of addressing data mismatch.\n",
    "\n",
    "### Diagnosis and Root Cause Analysis\n",
    "\n",
    "When error analysis (Dev Error - Training-Dev Error) indicates a significant **data mismatch problem**, the first step is to gain human insight into the distribution difference.\n",
    "\n",
    "* **Manual Error Analysis:** Manually examine misclassified examples in the **Dev Set** (not the Test Set, to avoid overfitting the final evaluation).\n",
    "* **Identify Differences:** Try to understand *how* the Dev Set data is different or harder than the Training Set data.\n",
    "    * *Example (Speech Recognition):* The Dev Set contains a high frequency of **car noise** (a new source of error) or requires accurate recognition of **street numbers** (a new priority).\n",
    "\n",
    "### Strategic Solutions (Fixing the Mismatch)\n",
    "\n",
    "Once the cause is identified, the goal is to make the Training Data distribution more similar to the Dev/Test distribution. The two main approaches are:\n",
    "\n",
    "| Strategy | Action | Example |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data Collection** | Deliberately collect more real data that matches the Dev/Test distribution features. | Collect more audio recordings of people speaking street addresses. |\n",
    "| **Artificial Data Synthesis (ADS)** | Programmatically manipulate existing clean data to simulate the hard-to-collect noise/features found in the Dev Set. | Synthesize in-car noise by adding separately recorded car audio to large amounts of clean speech audio. |\n",
    "\n",
    "### Cautions and Risks of Artificial Data Synthesis (ADS)\n",
    "\n",
    "While ADS can provide significant performance boosts, it carries a major risk: **Overfitting to the Synthesized Features.**\n",
    "\n",
    "* **Risk of Impoverished Subset:** If a large dataset of clean data (e.g., 10,000 hours of speech) is combined with a very small, unique noise source (e.g., 1 hour of car noise) that is simply repeated, the model may overfit to the subtle patterns of that single hour of noise.\n",
    "* **The Human Perception Trap:** The synthesized data may sound perfectly fine to a human ear, but the algorithm might be synthesizing data from only a tiny, unrepresentative subset of the total possible noise space.\n",
    "* **Best Practice:** Ideally, the unique component being added (e.g., car noise) should also be highly unique and varied (e.g., 10,000 unique hours of car noise) to match the scale of the clean data and prevent the model from overfitting to specific noise artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8311c16",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
