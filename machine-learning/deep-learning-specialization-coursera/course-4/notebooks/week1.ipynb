{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f162c9e2",
   "metadata": {},
   "source": [
    "# Week 1: Foundations of Conventional Neural Netweorks\n",
    "\n",
    "Implement the foundational layers of CNNs (pooling, convolutions) and stack them properly in a deep network to solve multi-class image classification problems.\n",
    "\n",
    "**Learning Objectives**:\n",
    "\n",
    "* Explain the convolution operation\n",
    "* Apply two different types of pooling operations\n",
    "* Identify the components used in a convolutional neural network (padding, stride, filter, ...) and their purpose\n",
    "* Build a convolutional neural network\n",
    "* Implement convolutional and pooling layers in numpy, including forward propagation\n",
    "* Implement helper functions to use when implementing a TensorFlow model\n",
    "* Create a mood classifer using the TF Keras Sequential API\n",
    "* Build a ConvNet to identify sign language digits using the TF Keras Functional API\n",
    "* Build and train a ConvNet in TensorFlow for a binary classification problem\n",
    "* Build and train a ConvNet in TensorFlow for a multiclass classification problem\n",
    "* Explain different use cases for the Sequential and Functional APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da84dfc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5600f",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c610b3d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0092d2f",
   "metadata": {},
   "source": [
    "## Computer Vision\n",
    "\n",
    "In this introduction to Convolutional Neural Networks (CNNs), the focus is on the transformative power of computer vision and the technical necessity of convolutions when dealing with high-resolution image data.\n",
    "\n",
    "### Importance and Impact of Computer Vision\n",
    "\n",
    "* **Rapid Advancement:** Deep learning has propelled computer vision into real-world utility, enabling self-driving cars, advanced face recognition, and relevant content curation in consumer apps.\n",
    "* **Cross-Fertilization:** Architectural innovations in computer vision often inspire breakthroughs in other fields, such as speech recognition.\n",
    "\n",
    "### Key Computer Vision Problems\n",
    "\n",
    "* **Image Classification:** Determining whether an object (e.g., a cat) is present in an image.\n",
    "* **Object Detection:** Not only identifying objects but also determining their specific positions and drawing bounding boxes around them.\n",
    "* **Neural Style Transfer:** Repainting a content image in the artistic style of a reference image (e.g., turning a landscape photo into a \"Picasso\" style painting).\n",
    "\n",
    "<img src='images/cv.png' width=750px>\n",
    "\n",
    "### The Challenge of Input Scale\n",
    "\n",
    "* **Small Images:** A $64 \\times 64$ RGB image has 12,288 features ($64 \\times 64 \\times 3$), which is manageable for standard fully connected networks.\n",
    "* **Large Images:** A modest $1000 \\times 1000$ (1-megapixel) image results in 3,000,000 input features.\n",
    "* **Parameter Explosion:** In a fully connected layer with just 1,000 hidden units, a 1-megapixel input would require a weight matrix with 3 billion parameters.\n",
    "* **Overfitting:** With billions of parameters, models are highly prone to overfitting without massive amounts of data.\n",
    "* **Resource Constraints:** The memory and computational power required to train such a network are generally infeasible for standard hardware.\n",
    "\n",
    "<img src='images/input_scale.png' width=750px>\n",
    "\n",
    "### The Solution: Convolutional Operations\n",
    "\n",
    "* To process high-resolution images efficiently without a parameter explosion, deep learning uses **Convolutional Neural Networks (CNNs)**.\n",
    "* **Convolutions:** This operation is the fundamental building block of CNNs, allowing the network to learn local patterns (like edges) while drastically reducing the number of parameters compared to fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1f1d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8a309",
   "metadata": {},
   "source": [
    "## Edge Detection Example\n",
    "\n",
    "The convolution operation is a fundamental building block of Convolutional Neural Networks (CNNs). It allows a model to learn features—starting with simple edges and progressing to complex objects—by sliding a filter over an input image.\n",
    "\n",
    "### What is the Convolution Operation?\n",
    "\n",
    "In computer vision, convolution is used to detect specific features, such as vertical or horizontal lines.\n",
    "* **The Input:** A grayscale image is represented as a matrix of pixel intensities (e.g., a $6 \\times 6 \\times 1$ matrix).\n",
    "* **The Filter (or Kernel):** A smaller matrix (typically $3 \\times 3$) designed to identify a specific pattern. For a vertical edge detector, a common filter is:\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1 \\end{bmatrix}$$\n",
    "\n",
    "* **The Notation:** In math and deep learning, the asterisk ($*$) denotes the convolution operation (not to be confused with standard multiplication).\n",
    "\n",
    "### The Mechanics of Convolution\n",
    "\n",
    "The process of convolving a $6 \\times 6$ image with a $3 \\times 3$ filter results in a $4 \\times 4$ output matrix.\n",
    "\n",
    "1. **Overlay:** Place the $3 \\times 3$ filter over the top-left $3 \\times 3$ patch of the image.\n",
    "2. **Element-wise Product:** Multiply each of the 9 numbers in the filter by the corresponding pixel value in the image patch.\n",
    "3. **Summation:** Add those 9 products together to get a single value for the first cell of the output matrix.\n",
    "4. **Shift (Slide):** Move the filter one pixel to the right (the \"stride\") and repeat the calculation. Once the row is finished, move down and start the next row.\n",
    "\n",
    "### Intuition: Why It Detects Edges\n",
    "\n",
    "The filter acts as a mathematical \"transition detector.\"\n",
    "* **Vertical Edge Case:** Imagine an image where the left half is bright (pixel value 10) and the right half is dark (pixel value 0).\n",
    "* **The Calculation:** When the filter (with 1s on the left and -1s on the right) sits on the transition:\n",
    "    * The 1s multiply the bright pixels (high positive sum).\n",
    "    * The -1s multiply the dark pixels (near-zero sum).\n",
    "* **The Result:** The sum is a large positive number (e.g., 30). In areas where the color is uniform (all 10s or all 0s), the 1s and -1s cancel each other out, resulting in 0.\n",
    "* **Visual Output:** The final $4 \\times 4$ matrix will show a bright \"strip\" in the middle, representing the detected edge.\n",
    "\n",
    "<img src='images/edge_detection.png' width=750px>\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "In deep learning frameworks, you don't perform these sums manually. Functions are built-in to handle high-dimensional convolutions:\n",
    "* **TensorFlow:** `tf.nn.conv2d`\n",
    "* **Keras:** `Conv2D layer`\n",
    "* **Output Dimensions:** For an $n \\times n$ image and an $f \\times f$ filter, the output size is generally $(n - f + 1) \\times (n - f + 1)$. In our example, $6 - 3 + 1 = 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34e7c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c9546",
   "metadata": {},
   "source": [
    "## More on Edge Detection\n",
    "\n",
    "This section discusses edge detection transitions, specialized filters, and the transition to learned parameters in Convolutional Neural Networks (CNNs).\n",
    "\n",
    "### Positive vs. Negative Edge Transitions\n",
    "\n",
    "* **Direction Matters:** Edge detection identifies the direction of light intensity change.\n",
    "    * **Light to Dark:** A transition from high pixel values ($10$) to low values ($0$) results in a positive output (e.g., $+30$).\n",
    "    * **Dark to Light:** A transition from low pixel values ($0$) to high values ($10$) results in a negative output (e.g., $-30$).\n",
    "* **Absolute Value:** If the specific direction of the transition is irrelevant to the task, the absolute value of the output matrix can be used to treat all edges equally.\n",
    "\n",
    "### Horizontal vs. Vertical Detection\n",
    "\n",
    "* **Vertical Filters:** Designed with vertical columns of weights (e.g., positive on left, negative on right) to detect changes across the x-axis.\n",
    "* **Horizontal Filters:** Designed with horizontal rows of weights to detect changes across the y-axis (top vs. bottom). A typical horizontal filter is the 90-degree rotation of the vertical filter:\n",
    "\n",
    "$$\\text{Horizontal Filter} = \\begin{bmatrix}1&1&1\\\\0&0&0\\\\-1&-1&-1\\end{bmatrix}$$\n",
    "\n",
    "### Specialized Hand-Coded Filters\n",
    "\n",
    "Historically, researchers developed specific matrices to make edge detection more robust to noise:\n",
    "* **Sobel Filter:** Adds weight to the central pixel to increase robustness.\n",
    "\n",
    "$$\\text{Sobel} = \\begin{bmatrix}1&0&-1\\\\2&0&-2\\\\1&0&-1\\end{bmatrix}$$\n",
    "\n",
    "* **Scharr Filter:** Uses even more aggressive weighting for specific statistical properties.\n",
    "\n",
    "$$\\text{Scharr} = \\begin{bmatrix}3&0&-3\\\\10&0&-10\\\\3&0&-3\\end{bmatrix}$$\n",
    "\n",
    "### The Deep Learning Paradigm: Learned Filters\n",
    "\n",
    "The most significant shift in modern AI is moving from hand-coded filters to learned parameters.\n",
    "\n",
    "**Learnable Weights:** Instead of manually picking values like $1$ or $10$, the nine numbers in a $3\\times3$ filter are treated as parameters ($w_1$ through $w_9$). The network uses backpropagation to automatically learn the optimal weights based on the dataset. Learned filters can detect edges at any orientation (e.g., $45^\\circ$, $73^\\circ$) or even complex textures that do not have a standard mathematical name.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "Whether hand-coded or learned, the operation remains a convolution. For a $6\\times6$ input and a $3\\times3$ filter, we are optimizing the parameters to produce a $4\\times4$ feature map that minimizes the loss function:\n",
    "\n",
    "$$(n-f+1) \\times (n-f+1) = (6-3+1) \\times (6-3+1) = 4 \\times 4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d74859",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e6ee5",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "This section describes the key principles of Padding, which is a vital modification to the standard convolution operation used to build deep neural networks.\n",
    "\n",
    "### The Problems with \"Valid\" Convolutions\n",
    "\n",
    "Without padding, standard convolutions (called \"Valid\" convolutions) suffer from two primary issues:\n",
    "* **Image Shrinkage:** Every layer reduces the spatial dimensions. In deep networks with hundreds of layers, the image would shrink to $1\\times{1}$ very quickly.\n",
    "* **Loss of Edge Information:** Pixels at the corners and edges are only \"touched\" by the filter a few times, whereas central pixels are included in many overlapping $3\\times3$ regions. This results in the network effectively \"throwing away\" data from the borders.\n",
    "\n",
    "### The Solution: Padding ($p$)\n",
    "\n",
    "Padding involves adding a border of pixels (usually filled with zeros) around the original image before applying the filter.\n",
    "* **Dimensionality Preservation:** If we pad a $6\\times6$ image with a border of $p=1$, it becomes an $8\\times8$ matrix. Convolving this with a $3\\times3$ filter results in a $6\\times6$ output, perfectly preserving the original size.\n",
    "* **Updated Output Formula:** With padding $p$ included, the output dimension for an $n\\times{n}$ image and $f\\times{f}$ filter is:\n",
    "\n",
    "$$(n+2p-f+1)\\times(n+2p-f+1)$$\n",
    "\n",
    "Example: For $n=6, f=3, p=1$:\n",
    "\n",
    "$$(6+2(1)-3+1)=6$$\n",
    "\n",
    "### Common Padding Choices\n",
    "1. **Valid Convolution:** This means $p=0$. The output size is simply $(n-f+1)$.\n",
    "2. **Same Convolution:** The padding is calculated so that the output size equals the input size.\n",
    "To achieve a \"Same\" convolution, the padding $p$ must follow this formula:\n",
    "\n",
    "$$p=\\frac{f-1}{2}$$\n",
    "\n",
    "### Why Filters are Almost Always Odd\n",
    "\n",
    "In computer vision, filters ($f$) like $3\\times3$, $5\\times5$, or $7\\times7$ are standard. Even-sized filters are rarely used for two reasons:\n",
    "* **Symmetry:** If $f$ is even, the padding $p$ would need to be asymmetric (e.g., padding more on the left than the right), which is computationally messy.\n",
    "* **Central Pixel:** Odd-sized filters have a clear \"central pixel,\" which is helpful for tracking the position and orientation of features in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5181a637",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675145f7",
   "metadata": {},
   "source": [
    "## Strided Convolutions\n",
    "\n",
    "This section discusses the key mechanics of **Strided Convolutions** and the technical distinction between convolution and cross-correlation.\n",
    "\n",
    "### Strided Convolutions\n",
    "\n",
    "Strided convolutions introduce a \"jump\" in the sliding window process, effectively downsampling the image during the convolution itself.\n",
    "* **The Mechanism:** Instead of moving the filter by $1$ pixel at a time, we move it by a value $s$ (the stride).\n",
    "If $s=2$, the filter hops over two positions horizontally and vertically, skipping intermediate calculations.\n",
    "* **Purpose:** This is primarily used to reduce the spatial dimensions (width and height) of the feature maps as the data moves deeper into the network, reducing computational load and increasing the receptive field.\n",
    "\n",
    "### The General Output Dimension Formula\n",
    "\n",
    "When combining input size ($n$), filter size ($f$), padding ($p$), and stride ($s$), the dimension of the output is calculated as:\n",
    "\n",
    "$$\\lfloor\\frac{n+2p-f}{s}+1\\rfloor\\times\\lfloor\\frac{n+2p-f}{s}+1\\rfloor$$\n",
    "\n",
    "The Floor Function: The $\\lfloor{z}\\rfloor$ notation denotes the \"floor\" of $z$, which means rounding down to the nearest integer.\n",
    "\n",
    "**The Logic:** In deep learning conventions, a filter must lie entirely within the image (or padded area). If the stride causes the filter to \"hang\" off the edge, that final calculation is simply discarded.\n",
    "\n",
    "Example: For $n=7, f=3, p=0, s=2$:\n",
    "\n",
    "$$\\lfloor\\frac{7+0-3}{2}+1\\rfloor=\\lfloor\\frac{4}{2}+1\\rfloor=3$$\n",
    "\n",
    "The resulting output is a $3\\times3$ matrix.\n",
    "\n",
    "### Convolution vs. Cross-Correlation\n",
    "\n",
    "There is a common terminology mismatch between pure mathematics and deep learning:\n",
    "* **Mathematical Convolution:** Requires \"flipping\" the filter both horizontally and vertically (mirroring) before performing the element-wise product. This gives the operation the associative property:\n",
    "\n",
    "$$(A*B)*C=A*(B*C)$$\n",
    "\n",
    "* **Cross-Correlation:** The exact same operation but without the flipping step.\n",
    "* **Deep Learning Convention:** In AI literature, we perform cross-correlation but almost universally refer to it as \"convolution.\"\n",
    "* **Why it doesn't matter:** In a neural network, the filter values are learned via backpropagation. If a \"flipped\" filter were better, the network would simply learn the weights in that flipped orientation automatically. Omitting the flip simplifies implementation without affecting performance.\n",
    "\n",
    "### Summary Table of Parameters\n",
    "| Parameter | Symbol | Effect on Output Size |\n",
    "| --- | --- | --- |\n",
    "|Input Size | $n$ | Larger input leads to larger output |\n",
    "| Filter Size | $f$ | Larger filter reduces output size |\n",
    "| Padding | $p$ | Increases output size (preserves borders)| \n",
    "| Stride | $s$ | Larger stride significantly reduces output size |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0a3be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a54d6b",
   "metadata": {},
   "source": [
    "## Convolutions Over Volume\n",
    "\n",
    "This section explains how the convolution operation expands from 2D grayscale images into 3D Volumes, which is the standard way CNNs process color images and complex feature maps.\n",
    "\n",
    "### Convolving over RGB Channels\n",
    "\n",
    "* **The Input Volume:** A color image is typically represented as a $6 \\times 6 \\times 3$ volume, where the last dimension corresponds to the Red, Green, and Blue (RGB) channels.\n",
    "* **The 3D Filter:** To convolve over this volume, the filter must also be 3D. A $3 \\times 3$ filter becomes a $3 \\times 3 \\times 3$ volume.\n",
    "* **Channel Matching Rule:** The number of channels in the filter must exactly match the number of channels in the input.\n",
    "    * Formula: $n \\times n \\times n_c$ input requires an $f \\times f \\times n_c$ filter.\n",
    "* **The Calculation:** The filter (containing $27$ parameters for $3^3$) overlays a 3D patch of the image. You perform an element-wise product of all $27$ numbers and sum them into a single value.\n",
    "* **Dimensionality Reduction:** Even though the input and filter are 3D, the output of a single filter is a 2D matrix.\n",
    "Output size: $(n - f + 1) \\times (n - f + 1) \\times 1$.\n",
    "\n",
    "### Multiple Feature Detection\n",
    "\n",
    "In practice, we don't just want to detect one type of feature (like vertical edges). We want to detect many features simultaneously.\n",
    "* **Stacking Filters:** If you use two different filters (e.g., one for vertical edges and one for horizontal edges), each produces its own $4 \\times 4$ output.\n",
    "* **Output Volumes:** These 2D outputs are stacked together to form a new 3D volume.\n",
    "If you use $2$ filters, the output is $4 \\times 4 \\times 2$.\n",
    "If you use $128$ filters, the output is $4 \\times 4 \\times 128$.\n",
    "* **Channels vs. Depth:** While some literature refers to this third dimension as \"depth,\" the term \"channels\" is preferred in this context to avoid confusion with the \"depth\" (number of layers) of the neural network itself.\n",
    "\n",
    "<img src='images/con_vol.png' width=750px>\n",
    "\n",
    "### Summary of Dimensions\n",
    "\n",
    "For an input volume with height/width $n$ and $n_c$ channels, using $n_c'$ filters of size $f$:\n",
    "\n",
    "**Input:**\n",
    "\n",
    "$$n \\times n \\times n_c$$\n",
    "\n",
    "**Filter:**\n",
    "\n",
    "$$f \\times f \\times n_c'$$\n",
    "\n",
    "**Output:**\n",
    "\n",
    "$$(n - f + 1) \\times (n - f + 1) \\times n_c'$$\n",
    "\n",
    "(Note: This assumes a stride $s = 1$ and padding $p = 0$. If these are changed, the spatial dimensions $n - f + 1$ are updated using the standard formulas discussed previously.)\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "This approach allows the network to look for different patterns in different colors (e.g., an edge that only appears in the Red channel) or look for universal patterns by using the same weights across all three channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f7081b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc8f39",
   "metadata": {},
   "source": [
    "## One Layer of Convolutional Network\n",
    "\n",
    "In this section, we demonstrate the transition from a single convolution operation to a complete Convolutional Neural Network (CNN) Layer. This involves adding biases, non-linearities, and understanding parameter scaling.\n",
    "\n",
    "### Building a Full CNN Layer\n",
    "\n",
    "One layer of a CNN consists of several sequential operations that map an input volume $a^{[l-1]}$ to an output activation volume $a^{[l]}$:\n",
    "1. **Convolution:** Multiply the input volume by $n_c$ different filters (weights $W^{[l]}$).\n",
    "2. **Bias Addition:** Add a specific bias $b^{[l]}$ to each filter's output. Note that through broadcasting, a single real number is added to every element in that filter's output map.\n",
    "3. **Non-linearity:** Apply an activation function, most commonly ReLU ($g(z) = \\max(0, z)$).\n",
    "4. **Stacking:** Combine the results into a 3D volume where the depth equals the number of filters used.\n",
    "\n",
    "<img src='images/cnn_one_layer_example.png' width=800px>\n",
    "\n",
    "### Parameter Calculation Exercise\n",
    "\n",
    "A major advantage of CNNs is parameter sharing, which makes them less prone to overfitting than fully connected networks.\n",
    "\n",
    "**Example Case:**\n",
    "\n",
    "* **Input:** $6 \\times 6 \\times 3$ image\n",
    "* **Filters:** $10$ filters of size $3 \\times 3 \\times 3$\n",
    "* **Parameters per filter:** $(3 \\times 3 \\times 3) = 27$ weights $+ 1$ bias = $28$ parameters.\n",
    "* **Total parameters for the layer:** $28 \\times 10 = 280$\n",
    "\n",
    "**Key Insight:** The number of parameters ($280$) remains constant regardless of the input image size (whether it's $1,000 \\times 1,000$ or $5,000 \\times 5,000$).\n",
    "\n",
    "### Notation Summary for Layer $l$\n",
    "\n",
    "To describe deep networks, we use specific notation for each layer's properties:\n",
    "* **Filter Size:** $f^{[l]}$\n",
    "* **Padding:** $p^{[l]}$\n",
    "* **Stride:** $s^{[l]}$\n",
    "* **Number of Filters:** $n_c^{[l]}$\n",
    "\n",
    "**Dimensions of Volumes:**\n",
    "* **Input:** $n_H^{[l-1]} \\times n_W^{[l-1]} \\times n_c^{[l-1]}$\n",
    "* **Output:** $n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$\n",
    "\n",
    "The height ($n_H$) and width ($n_W$) are calculated using:\n",
    "\n",
    "$$n_H^{[l]} = \\lfloor \\frac{n_H^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \\rfloor$$\n",
    "\n",
    "**Weights and Activations:**\n",
    "* **Weights ($W^{[l]}$):** The total volume of weights is $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]} \\times n_c^{[l]}$.\n",
    "* **Biases ($b^{[l]}$):** A vector of dimension $n_c^{[l]}$ (or represented as $1 \\times 1 \\times 1 \\times n_c^{[l]}$).\n",
    "* **Activations ($a^{[l]}$):** $n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$.\n",
    "\n",
    "In vectorized implementations (batch size or mini-batch size $m$), the dimension of $A^{[l]}$ is $m \\times n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$.\n",
    "\n",
    "**Note on Convention:** While some frameworks use \"channels first\" ($m, n_c, n_H, n_W$), here we follow the \"channels last\" convention ($m, n_H, n_W, n_c$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc70106",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b027918",
   "metadata": {},
   "source": [
    "## Simple Convolutional Netwrok Example\n",
    "\n",
    "This section describes the architecture of a complete Deep Convolutional Neural Network (ConvNet) example. This example demonstrates how spatial dimensions shrink while feature depth increases as data moves toward a final classification.\n",
    "\n",
    "### Anatomy of a Typical ConvNet\n",
    "\n",
    "A standard image classification pipeline follows a specific progression: taking an input image $x$ (e.g., $39 \\times 39 \\times 3$) and transforming it into a high-level feature vector for prediction.\n",
    "* **Layer 1 (Conv1):**\n",
    "    * **Input:** $39 \\times 39 \\times 3$\n",
    "    * **Hyperparameters:** $f^{[1]}=3, s^{[1]}=1, p^{[1]}=0$ (Valid), $10$ filters.\n",
    "    * **Output:** $37 \\times 37 \\times 10$.\n",
    "* **Layer 2 (Conv2):**\n",
    "    * **Input:** $37 \\times 37 \\times 10$\n",
    "    * **Hyperparameters:** $f^{[2]}=5, s^{[2]}=2, p^{[2]}=0, 20$ filters.\n",
    "    * **Output:** $17 \\times 17 \\times 20$.\n",
    "* **Layer 3 (Conv3):**\n",
    "    * **Input:** $17 \\times 17 \\times 20$\n",
    "    * **Hyperparameters:** $f^{[3]}=5, s^{[3]}=2, p^{[3]}=0, 40$ filters.\n",
    "    * **Output:** $7 \\times 7 \\times 40$.\n",
    "\n",
    "<img src='images/conv_net_example.png' width=800px>\n",
    "\n",
    "### The Design Pattern: Spatial vs. Depth\n",
    "\n",
    "A key observation in ConvNet design is the inverse relationship between spatial dimensions and channel depth:\n",
    "* **Height and Width ($n_H, n_W$):** Typically decrease as you go deeper into the network (e.g., $39 \\to 37 \\to 17 \\to 7$).\n",
    "* **Number of Channels ($n_c$):** Typically increase as you go deeper (e.g., $3 \\to 10 \\to 20 \\to 40$). This allows the network to detect a wider variety of complex features even as the resolution coarsens.\n",
    "\n",
    "### Final Classification: Flattening and FC\n",
    "\n",
    "Once the features are extracted into a small 3D volume ($7 \\times 7 \\times 40$):\n",
    "* **Flattening:** The volume is unrolled into a single long vector.\n",
    "* **Total Units:** $7 \\times 7 \\times 40 = 1,960$ units.\n",
    "* **Fully Connected (FC):** This vector is fed into standard neural network layers (Logistic Regression or Softmax).\n",
    "Output: A final prediction $\\hat{y}$ (e.g., $0$ or $1$ for \"cat vs. no cat\").\n",
    "\n",
    "### Dimensionality Recap Formula\n",
    "\n",
    "To compute the spatial size for any layer, we use the generalized formula:\n",
    "\n",
    "$$n^{[l]} = \\lfloor \\frac{n^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \\rfloor$$\n",
    "\n",
    "For the transition from Layer 1 to Layer 2 in the example:\n",
    "\n",
    "$$\\lfloor \\frac{37 + 0 - 5}{2} + 1 \\rfloor = \\lfloor \\frac{32}{2} + 1 \\rfloor = 17$$\n",
    "\n",
    "### Three Types of ConvNet Layers\n",
    "\n",
    "Most modern architectures are composed of three primary building blocks:\n",
    "* **Convolutional (Conv):** Extracts features using sliding filters.\n",
    "* **Pooling (Pool):** Reduces spatial size to decrease computation and parameters (to be discussed next).\n",
    "* **Fully Connected (FC):** Performs the final high-level reasoning and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c06b31",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0c892",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "This section briefly touches on Pooling layers, which serve as a critical tool for reducing the spatial size of representations and making feature detection more robust.\n",
    "\n",
    "### The Mechanics of Pooling\n",
    "\n",
    "Pooling layers slide a window across the input volume, similar to a convolution, but instead of a dot product with weights, they perform a fixed mathematical operation.\n",
    "* **Max Pooling:** The most common type. Within each window (e.g., $2\\times2$), only the maximum value is preserved.\n",
    "    * **Intuition:** A high value represents the presence of a specific feature (like a cat's eye). If that feature is detected anywhere in the window, the max operation preserves that \"signal.\"\n",
    "* **Average Pooling:** Computes the mean value of all pixels in the window.\n",
    "    * **Usage:** While less common than Max Pooling, it is sometimes used very deep in a network to collapse spatial dimensions (e.g., from $7\\times7\\times1000$ to $1\\times1\\times1000$).\n",
    "\n",
    "### Key Properties and Hyperparameters\n",
    "\n",
    "Unlike convolutional layers, pooling layers are \"static\" operations.\n",
    "* **No Learnable Parameters:** There are no weights ($W$) or biases ($b$) for gradient descent to update. Once you choose the hyperparameters, the computation is fixed.\n",
    "* **Hyperparameters:**\n",
    "    * **Filter Size ($f$):** Common choice is $f=2$ or $f=3$.\n",
    "    * **Stride ($s$):** Common choice is $s=2$.\n",
    "    * **Padding ($p$):** Very rarely used; usually $p=0$.\n",
    "* **Channel Independence:** Pooling is applied to each channel ($n_c$) independently. The number of channels in the output is always identical to the number of channels in the input.\n",
    "\n",
    "### Dimensionality and Formulas\n",
    "\n",
    "The same formula used for convolutions applies to pooling to determine the output dimensions ($n_H \\times n_W \\times n_c$):\n",
    "\n",
    "Output Height/Width:\n",
    "\n",
    "$$\\lfloor\\frac{n+2p-f}{s}+1\\rfloor$$\n",
    "\n",
    "Example ($f=2, s=2$):\n",
    "\n",
    "A $4\\times4\\times n_c$ input pooled with $f=2, s=2$ results in a $2\\times2\\times n_c$ output. This effectively halves the height and width, reducing the total spatial area by $75\\%$.\n",
    "\n",
    "<img src='images/max_pooling.png' width=750px>\n",
    "\n",
    "### Why Use Pooling?\n",
    "\n",
    "* **Computational Efficiency:** By shrinking $n_H$ and $n_W$, it reduces the number of operations required in subsequent layers.\n",
    "* **Robustness:** It provides a form of translational invariance. If a feature shifts by a pixel or two, the \"max\" in that local region will likely stay the same.\n",
    "* **Overfitting Prevention:** By reducing the number of total activations, it helps the network focus on the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe7077",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde18312",
   "metadata": {},
   "source": [
    "## Neural Network Example\n",
    "\n",
    "This section demonstrates the architectural integration of a full Convolutional Neural Network (ConvNet). This example follows the classic logic of the LeNet-5 architecture, which is a foundational model for handwritten digit recognition.\n",
    "\n",
    "### ConvNet Layer Convention\n",
    "\n",
    "There is a common point of confusion in AI literature regarding what constitutes a \"layer\":\n",
    "* **The Weight Rule:** Most practitioners only count layers that contain learnable parameters (weights and biases).\n",
    "* **Grouping:** Because Pooling layers have no weights, they are typically grouped with the preceding Convolutional layer. Thus, \"Layer 1\" is often defined as $(\\text{Conv1} + \\text{Pool1})$.\n",
    "\n",
    "### The Progressive Architecture (Example Walkthrough)\n",
    "\n",
    "A standard network follows a repeating pattern of feature extraction followed by classification.\n",
    "1. **Feature Extraction (Layers 1 & 2):**\n",
    "    * **Layer 1:** A $32\\times32\\times3$ input is convolved with $6$ filters ($5\\times5$), resulting in $28\\times28\\times6$. It is then downsampled via Max Pooling ($f=2, s=2$) to $14\\times14\\times6$.\n",
    "    * **Layer 2:** A second convolution with $16$ filters ($5\\times5$) results in $10\\times10\\times16$, which is pooled down to $5\\times5\\times16$.\n",
    "2. **Flattening:** The final $5\\times5\\times16$ volume is \"unrolled\" into a 1D vector of $400$ units ($5\\times5\\times16 = 400$).\n",
    "3. **Classification (Fully Connected Layers):**\n",
    "    * **FC3:** $400$ units connect to $120$ neurons.\n",
    "    * **FC4:** $120$ units connect to $84$ neurons.\n",
    "    * **Output Layer:** $84$ units connect to a Softmax layer with $10$ outputs (representing digits $0$ through $9$).\n",
    "\n",
    "<img src='images/cnn_example.png' width=900px>\n",
    "\n",
    "### Parameter vs. Activation Trends\n",
    "\n",
    "Analyzing the data flow of a ConvNet reveals several critical patterns:\n",
    "* **Spatial Dimensions ($n_H, n_W$):** Gradually decrease as you go deeper ($32 \\to 28 \\to 14 \\to 10 \\to 5$).\n",
    "* **Channel Depth ($n_c$):** Gradually increases ($3 \\to 6 \\to 16$).\n",
    "* **Parameter Distribution:**\n",
    "    * **Conv Layers:** Use relatively few parameters due to weight sharing.\n",
    "    * **FC Layers:** Contain the vast majority of the network's parameters.\n",
    "    * **Pool Layers:** Contain exactly $0$ parameters.\n",
    "* **Activation Size:** Ideally should decrease gradually. A sudden \"drop\" in activation size can lead to a loss of representational power.\n",
    "\n",
    "### Dimensionality Summary Table\n",
    "\n",
    "For an input $32\\times32\\times3$, the transformation looks like this:\n",
    "\n",
    "| Layer | Type | Activation Shape | Activation Size | Parameters |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Input | Image | $32 \\times 32 \\times 3$ | 3,072 | $0$ |\n",
    "| Layer 1 | Conv | $28 \\times 28 \\times 6$ | 4,704 | $(\\text{f} \\cdot \\text{f} \\cdot 3 + 1) \\cdot 6 = 456$|\n",
    "| Layer 1 | Pool | $14 \\times 14 \\times 6$ | 1,176 | 0 |\n",
    "| Layer 2 | Conv | $10 \\times 10 \\times 16$ | 1,600 | $(\\text{f} \\cdot \\text{f} \\cdot 6 + 1) \\cdot 16 = 2,416$ |\n",
    "| Layer 2 | Pool | $5 \\times 5 \\times 16$ | 400 | 0 |\n",
    "| FC 3 | Dense | $120 \\times 1$ | 120 |$400 \\cdot 120 + 120 = 48,120$ |\n",
    "| FC 4 | Dense | $84 \\times 1$ | 84 | $120 \\cdot 84 + 84 = 10,164$ |\n",
    "| Output | Softmax | $10 \\times 1$ | 10 | $84 \\cdot 10 + 10 = 850$|\n",
    "\n",
    "### Expert Advice\n",
    "\n",
    "When designing your own network, don't try to \"guess\" these hyperparameters ($f, s, p, n_c$) from scratch. The industry standard is to start with a proven architecture (like LeNet, AlexNet, or VGG) that has worked on similar datasets and adapt it to your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34485bd9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15eb60",
   "metadata": {},
   "source": [
    "## Why Convolutions?\n",
    "\n",
    "This section discusses the concluding concepts for this week regarding why Convolutional Neural Networks (CNNs) are mathematically and practically superior to standard networks for computer vision.\n",
    "\n",
    "### The Two Primary Advantages of Convolutions\n",
    "\n",
    "CNNs use significantly fewer parameters than Fully Connected (FC) networks, which makes them faster to train and less prone to overfitting.\n",
    "\n",
    "* **Parameter Sharing:** A feature detector (like a vertical edge detector) that is useful in the top-left of an image is likely useful in the bottom-right. Instead of learning a different detector for every pixel location, the CNN learns one set of filter weights and applies it across the entire image.\n",
    "    * *Example:* A $5\\times5$ filter has only $26$ parameters (including bias), regardless of the input image size.\n",
    "* **Sparsity of Connections:** In a convolutional layer, each output value depends only on a small local region of the input (the size of the filter).\n",
    "    * *Example:* In a $3\\times3$ convolution, one output unit is connected to only $9$ input pixels. In an FC layer, that same unit would be connected to every single pixel in the image.\n",
    "\n",
    "### Parameter Comparison Example\n",
    "\n",
    "Consider a $32\\times32\\times3$ input ($3,072$ units) transforming into a $28\\times28\\times6$ output ($4,704$ units).\n",
    "* Fully Connected Approach:\n",
    "\n",
    "$$3,072 \\times 4,704 \\approx 14,000,000\\text{ parameters}$$\n",
    "\n",
    "* Convolutional Approach ($6$ filters of $5\\times5$):\n",
    "\n",
    "$$(5 \\times 5 \\times 3 + 1) \\times 6 = 456\\text{ parameters}$$\n",
    "\n",
    "The CNN reduces the parameter count by more than $30,000$ times in this single layer transition.\n",
    "\n",
    "### Translation Invariance\n",
    "\n",
    "CNNs naturally capture Translation Invariance, which is the property that an object (like a cat) is still the same object even if it is shifted a few pixels in any direction. Because the network applies the same filters across all positions, it learns to recognize features regardless of their specific coordinates.\n",
    "\n",
    "### Training the Network\n",
    "\n",
    "Training a CNN follows the same fundamental principles as a standard Deep Neural Network:\n",
    "1. **Initialization:** Randomly initialize weights $W$ and biases $b$.\n",
    "2. **Forward Prop:** Pass the image through Conv, Pool, and FC layers to generate a prediction $\\hat{y}$.\n",
    "3. **Cost Function:** Calculate the total loss $J$ over $m$ training examples:\n",
    "\n",
    "$$J = \\frac{1}{m}\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})$$\n",
    "\n",
    "4. **Optimization:** Use algorithms like Gradient Descent, Momentum, RMSProp, or Adam to update the parameters and minimize the cost.\n",
    "\n",
    "### Final Pro Tip\n",
    "\n",
    "Due to the high number of hyperparameters in CNNs, the most effective strategy for most developers is **Transfer Learning** or **Architecture Adaptation**: using a structure that has already been proven effective in research (like ResNet or Inception) and fine-tuning it for your specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f7295",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
