{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a884b6a5-e9fa-4fc9-af71-689b47ed4a00",
   "metadata": {},
   "source": [
    "# Week 1: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c712d9-870f-49d7-8bf6-fd55ee063e0c",
   "metadata": {},
   "source": [
    "## What is Clustering?\n",
    "\n",
    "In this section we introduce the concept of **Clustering** as a foundational unsupervised learning algorithm, contrasting it with supervised learning.\n",
    "\n",
    "### 1. Definition and Goal\n",
    "* **Clustering Algorithm:** Looks at a number of data points and automatically finds data points that are **related or similar** to each other.\n",
    "* **Goal:** To **group data into clusters**—meaning groups of points that are cohesive and similar—to find interesting **structure** in the data.\n",
    "\n",
    "### 2. Supervised vs. Unsupervised Learning\n",
    "| Feature | Supervised Learning (e.g., Classification) | Unsupervised Learning (e.g., Clustering) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Input Data** | **Features $X$ and Target Labels $Y$** (the \"right answer\") | **Only Input Features $X$** (no labels $Y$) |\n",
    "| **Algorithm's Task** | To learn a function to map $X$ to the predicted label $\\hat{y}$ (e.g., a decision boundary). | To find hidden patterns or inherent structure within the data $X$. |\n",
    "| **Visual Example** | Data plotted with two distinct classes (e.g., X's and O's). | Data plotted as simple dots with no initial class distinction. |\n",
    "\n",
    "### 3. Applications of Clustering\n",
    "Clustering is used across many fields to automatically categorize data:\n",
    "\n",
    "* **News Aggregation:** Grouping similar news articles together (e.g., all stories about a particular topic).\n",
    "* **Market Segmentation:** Analyzing user data to group individuals who share similar needs, goals, or motivations (e.g., learners grouped by career development vs. staying updated on AI).\n",
    "* **Biological Analysis:** Analyzing DNA or genetic expression data to group individuals exhibiting similar traits.\n",
    "* **Astronomy:** Grouping astronomical bodies to identify which ones form coherent structures in space or belong to the same galaxy.\n",
    "\n",
    "The next topic introduced is the **K-Means algorithm**, the most common clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c584b0-313b-4092-b164-883a14a5b4f9",
   "metadata": {},
   "source": [
    "## K-means Intuition\n",
    "\n",
    "This section illustrates the mechanics of the **K-Means clustering algorithm** using a visual example, highlighting its core iterative steps.\n",
    "\n",
    "\n",
    "### Key Bullet Points: The K-Means Clustering Algorithm\n",
    "\n",
    "### 1. Goal and Initialization\n",
    "* **Goal:** To group a dataset into $K$ (in this example, $K=2$) distinct clusters based on similarity.\n",
    "* **Cluster Centroids:** The centers of the clusters are called **cluster centroids** (illustrated by crosses).\n",
    "* **Initialization (Step 1):** The algorithm begins by making a **random initial guess** for the locations of the $K$ cluster centroids.\n",
    "\n",
    "![K-means Initialization](images/kmeans1.png)\n",
    "\n",
    "### 2. The Two Core Iterative Steps\n",
    "K-Means repeatedly alternates between two main steps until convergence:\n",
    "\n",
    "#### A. Assignment Step (Assign Points to Centroids)\n",
    "* **Action:** The algorithm iterates through **every data point** in the dataset.\n",
    "* **Logic:** Each point is assigned to the cluster centroid to which it is **closest** (e.g., closer to the red cross or the blue cross).\n",
    "* **Result:** This partitions the dataset, effectively coloring (or labeling) each point according to its nearest centroid.\n",
    "\n",
    "#### B. Update Step (Move Centroids)\n",
    "* **Action:** The algorithm looks at the new assignments for each cluster.\n",
    "* **Logic:** Each cluster centroid is moved to the **mean (average) location** of all the data points currently assigned to it (e.g., move the red cross to the average location of all red dots).\n",
    "* **Result:** This generates a new, hopefully improved, guess for the location of the cluster centers.\n",
    "\n",
    "![K-means Convergence](images/kmeans2.png)\n",
    "\n",
    "### 3. Convergence\n",
    "* **Definition:** The K-Means algorithm has **converged** when repeating the Assignment and Update steps results in **no further changes** to:\n",
    "    1.  The **assignment** of points to clusters (the colors of the points stop changing).\n",
    "    2.  The **locations** of the cluster centroids.\n",
    "* **Outcome:** The final positions of the centroids and the final assignments define the resulting clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8fcc57-b4d8-4251-b9ee-8830bb5044bf",
   "metadata": {},
   "source": [
    "## K-means Algorithm\n",
    "\n",
    "This section details the formal steps of the **K-Means clustering algorithm**, including the mathematical formulation and handling of edge cases.\n",
    "\n",
    "### Key Bullet Points: Detailed K-Means Algorithm\n",
    "\n",
    "### 1. Initialization\n",
    "* **Random Initialization:** The first step is to randomly initialize $K$ cluster centroids, denoted as $\\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\dots, \\boldsymbol{\\mu}_K$.\n",
    "* **Dimension:** Each centroid vector $\\boldsymbol{\\mu}_k$ must have the same dimension (number of features) as the training examples $\\mathbf{x}^{(i)}$.\n",
    "\n",
    "### 2. The Iterative Loop (Repeat until Convergence)\n",
    "K-Means cycles through two main steps:\n",
    "\n",
    "#### A. Step 1: Assignment of Points to Centroids\n",
    "* **Goal:** Assign every training example $\\mathbf{x}^{(i)}$ to the cluster centroid it is closest to.\n",
    "* **Assignment Variable ($c^{(i)}$):** For each example $\\mathbf{x}^{(i)}$, set $c^{(i)}$ to the index (from $1$ to $K$) of the nearest cluster centroid $\\boldsymbol{\\mu}_k$.\n",
    "* **Mathematical Formula (Minimizing Squared Distance):**\n",
    "    $$c^{(i)} = \\underset{k}{\\operatorname{argmin}} \\left\\| \\mathbf{x}^{(i)} - \\boldsymbol{\\mu}_k \\right\\|^2$$\n",
    "    * **Note:** The squared Euclidean distance ($\\text{L2 norm}^2$) is minimized for computational convenience, as the cluster with the smallest squared distance is the same as the one with the smallest distance.\n",
    "\n",
    "#### B. Step 2: Move Cluster Centroids\n",
    "* **Goal:** Update the location of each cluster centroid based on the points currently assigned to it.\n",
    "* **Update Logic:** For each cluster $k$ (from 1 to $K$), the new location of the centroid $\\boldsymbol{\\mu}_k$ is set to the **average (mean)** of all training examples assigned to that cluster.\n",
    "* **Mathematical Formula:**\n",
    "    $$\\boldsymbol{\\mu}_k := \\frac{1}{|\\text{Cluster}_k|} \\sum_{i: c^{(i)}=k} \\mathbf{x}^{(i)}$$\n",
    "\n",
    "### 3. Handling the Corner Case\n",
    "* **Zero Points:** If a cluster $k$ has **zero training examples** assigned to it (meaning the denominator $|\\text{Cluster}_k| = 0$), the mean is undefined.\n",
    "* **Common Solution:** The most common practice is to **eliminate that cluster**, resulting in $K-1$ clusters. An alternative is to randomly reinitialize the unassigned centroid.\n",
    "\n",
    "### 4. K-Means on Non-Separated Data\n",
    "* **Utility:** K-Means is often useful even when the data does not naturally lie in well-separated groups.\n",
    "* **Example:** In a t-shirt sizing problem (based on height and weight), K-Means can be used to artificially partition the continuous spectrum of data into $K=3$ groups, providing representative average dimensions for designing Small, Medium, and Large sizes.\n",
    "\n",
    "![K-means Non-separated](images/kmeans3.png)\n",
    "\n",
    "The next topic is exploring the specific **cost function** K-Means optimizes, which helps explain its convergence property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66872d4a-10a7-4b50-bc79-32dbfbbaffc7",
   "metadata": {},
   "source": [
    "## Optimization Objective\n",
    "\n",
    "The K-Means algorithm, despite not using gradient descent, is an iterative optimization algorithm designed to minimize a specific cost function called the **Distortion Function**.\n",
    "\n",
    "### Key Bullet Points: The K-Means Cost Function\n",
    "\n",
    "### 1. The Distortion Function (Cost $J$)\n",
    "* **Definition:** The K-Means cost function, often called the **Distortion Function**, measures the average squared distance between every training example and the center of the cluster to which it has been assigned.\n",
    "* **Notation:**\n",
    "    * $c^{(i)}$: The index of the cluster assigned to training example $\\mathbf{x}^{(i)}$.\n",
    "    * $\\boldsymbol{\\mu}_{c^{(i)}}$: The location of the cluster centroid assigned to $\\mathbf{x}^{(i)}$.\n",
    "* **Formula:** The cost $J$ is a function of all cluster assignments ($\\mathbf{c}$) and all centroid locations ($\\boldsymbol{\\mu}$):\n",
    "\n",
    "$$J(\\mathbf{c}, \\boldsymbol{\\mu}) = \\frac{1}{M} \\sum_{i=1}^{M} \\left\\| \\mathbf{x}^{(i)} - \\boldsymbol{\\mu}_{c^{(i)}} \\right\\|^2$$\n",
    "\n",
    "* **Goal of K-Means:** Find the assignments $\\mathbf{c}$ and centroid locations $\\boldsymbol{\\mu}$ that **minimize** this average squared distance.\n",
    "\n",
    "### 2. K-Means Steps as Optimization\n",
    "The two iterative steps of the K-Means algorithm are specifically designed to reduce the cost function $J$ in an alternating fashion:\n",
    "\n",
    "#### A. Step 1: Assignment Phase (Minimizing $J$ with $\\boldsymbol{\\mu}$ Fixed)\n",
    "* **Goal:** Choose the assignments $c^{(i)}$ to minimize $J$, holding the centroid locations ($\\boldsymbol{\\mu}$) constant.\n",
    "* **Action:** To minimize the distance term $\\left\\| \\mathbf{x}^{(i)} - \\boldsymbol{\\mu}_{c^{(i)}} \\right\\|^2$, the point $\\mathbf{x}^{(i)}$ must be assigned to the **closest** centroid $\\boldsymbol{\\mu}_k$.\n",
    "* **Conclusion:** This assignment rule guarantees that the cost $J$ will either decrease or stay the same in this step.\n",
    "\n",
    "#### B. Step 2: Update Phase (Minimizing $J$ with $\\mathbf{c}$ Fixed)\n",
    "* **Goal:** Choose the centroid locations $\\boldsymbol{\\mu}_k$ to minimize $J$, holding the assignments ($c^{(i)}$) constant.\n",
    "* **Action:** For any given cluster, setting the centroid $\\boldsymbol{\\mu}_k$ to the **mean (average)** of all points assigned to that cluster is the optimal choice that minimizes the squared distance for those points.\n",
    "* **Conclusion:** This update rule guarantees that the cost $J$ will either decrease or stay the same in this step.\n",
    "\n",
    "### 3. Convergence and Debugging\n",
    "* **Guaranteed Convergence:** Because every step of K-Means is a step of minimization, the cost function $J$ is **guaranteed to converge** (i.e., it will never increase).\n",
    "* **Convergence Test:** K-Means is considered converged when the cost $J$ no longer decreases (or decreases below a tiny threshold) over an iteration, indicating no further significant changes in assignments or centroid locations.\n",
    "* **Debugging:** If the cost function $J$ ever increases during training, it indicates a **bug** in the implementation.\n",
    "\n",
    "The cost function $J$ is also useful for comparing the results of different random initializations of the centroids, which is the subject of the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d943c-578a-4b6d-b01f-b8adf5b26de3",
   "metadata": {},
   "source": [
    "## Initializing K-means\n",
    "\n",
    "This section discusses how to handle the initial step of **K-means clustering** — choosing the initial cluster centroids — and how to mitigate the risk of finding a poor local optimum by using **multiple random initializations**.\n",
    "\n",
    "### Key Bullet Points\n",
    "\n",
    "* **Initial Centroid Selection:** The most common and recommended way to choose the initial $K$ cluster centroids ($\\mu_1$ through $\\mu_K$) is to **randomly pick $K$ existing training examples** from the dataset.\n",
    "* **Constraint on $K$:** The number of clusters $K$ must generally be less than the number of training examples $m$ ($K < m$).\n",
    "* **Problem of Local Optima:** The K-means algorithm is guaranteed to converge, but it is not guaranteed to find the global minimum for the distortion cost function $J$. Poor initial centroid placement can cause the algorithm to get stuck in a local minimum, leading to a sub-optimal clustering result.\n",
    "* **Solution: Multiple Random Initializations:** To find a better result, the algorithm should be run multiple times (e.g., 50 to 1000 times) using different sets of randomly initialized centroids each time.\n",
    "* **Selection Criterion:** After running K-means to convergence for each initialization, you must **compute the distortion cost function $J$** for every resulting clustering. The final, best clustering solution is the one that yielded the **lowest value for the cost function $J$**.\n",
    "* **Recommendation:** Using multiple random initializations is a common and highly recommended technique that significantly increases the likelihood of finding a superior set of clusters with a lower distortion cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294304bf-fef8-458b-bb4c-38845a2bef71",
   "metadata": {},
   "source": [
    "## Choosing the number of clusters\n",
    "\n",
    "Here we discuss the challenge of choosing the optimal number of clusters, $K$, for the K-means algorithm, emphasizing that the \"correct\" value is often ambiguous in unsupervised learning.\n",
    "\n",
    "### Key Bullet Points\n",
    "\n",
    "* **Ambiguity of $K$:** For most clustering problems, the correct value of $K$ is genuinely ambiguous because K-means is an unsupervised learning algorithm (no \"right answers\" or labels are given). Different values of $K$ (e.g., 2, 3, or 4 clusters) can all be seen as valid interpretations of the same data.\n",
    "\n",
    "* **The Elbow Method (Academic Technique):**\n",
    "    * **Method:** Run K-means for a range of $K$ values (e.g., $K=1, 2, 3, \\dots$) and plot the Distortion Cost Function ($J$) versus $K$.\n",
    "    * **Goal:** Look for an \"elbow\" point where the decrease in $J$ begins to slow significantly. This point is suggested as the optimal $K$.\n",
    "    * **Critique:** This method is often unreliable in practice because many cost functions decrease smoothly without a clear, distinct elbow.\n",
    "\n",
    "* **Incorrect Method:** You should **not** choose $K$ simply to minimize the cost function $J$, as this will almost always result in choosing the largest possible value of $K$ (more clusters always reduce distortion).\n",
    "\n",
    "* **Practical Method (Evaluate Downstream Purpose):**\n",
    "    * The most effective way to choose $K$ is to evaluate how well the resulting clusters perform for the later, downstream purpose for which the clustering was intended.\n",
    "    * **Example (T-Shirt Sizing):** Run K-means with $K=3$ (Small, Medium, Large) and $K=5$ (XS, S, M, L, XL). The choice between $K=3$ and $K=5$ is determined by the business trade-off between better t-shirt fit (more sizes) and increased manufacturing/inventory cost (more sizes).\n",
    "    * **Example (Image Compression):** The choice of $K$ (number of colors) is a trade-off between image quality and compression size.\n",
    "\n",
    "* **Summary:** When choosing $K$ in practice, run K-means with a few different values and select the one that optimizes the trade-off inherent in the real-world application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5020981-a8fe-48f9-94aa-e78d98f267e4",
   "metadata": {},
   "source": [
    "## Finding unusual events\n",
    "\n",
    "This section introduces the concept of **Anomaly Detection** as a second type of unsupervised learning algorithm. It explains the core mechanism — density estimation — and provides several real-world applications.\n",
    "\n",
    "### Key Bullet Points\n",
    "\n",
    "* **Definition:** Anomaly detection algorithms learn from an **unlabeled dataset of normal events** (e.g., standard aircraft engines) to detect or flag **unusual or anomalous events** (e.g., a faulty engine).\n",
    "* **Data Collection:** The training data ($m$ examples) consists primarily of **normal events**, as anomalous events are rare. Each example is defined by a feature vector $\\mathbf{x}$ (e.g., heat generated, vibration intensity).\n",
    "* **Core Mechanism: Density Estimation:**\n",
    "    1.  The algorithm builds a model for the probability of $\\mathbf{x}$ ($p(\\mathbf{x})$) based on the normal training data, determining which feature values have high versus low probability.\n",
    "    2.  For a new test example ($\\mathbf{x}_{\\text{test}}$), the algorithm computes $p(\\mathbf{x}_{\\text{test}})$.\n",
    "    3.  **Anomaly Flag:** If $p(\\mathbf{x}_{\\text{test}})$ is less than a small threshold $\\epsilon$ (epsilon), the event is flagged as an anomaly.\n",
    "    4.  If $p(\\mathbf{x}_{\\text{test}})$ is greater than or equal to $\\epsilon$, the event is considered \"okay\" or normal.\n",
    "\n",
    "![Density Estimation](images/density_estimation.png)\n",
    "\n",
    "\n",
    "* **Applications:** Anomaly detection is widely used in commercially important fields:\n",
    "    * **Manufacturing:** Inspecting newly manufactured units (aircraft engines, circuit boards, smartphones) that behave strangely.\n",
    "    * **Fraud Detection:** Identifying unusual user activities (e.g., login frequency, typing speed, transaction patterns) to flag potential financial fraud or fake accounts.\n",
    "    * **IT Monitoring:** Checking computer features in data centers (memory usage, CPU load, disk access) to find machines that are malfunctioning or potentially compromised.\n",
    "* **Next Steps:** The implementation of anomaly detection typically involves using the **Gaussian Distribution** to model the data $p(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b46592-b222-4c31-98b6-533af76d6963",
   "metadata": {},
   "source": [
    "## Gaussian (Normal) distribution\n",
    "\n",
    "In this section, we introduce the **Gaussian (Normal) Distribution** as the foundational mathematical tool used to model data density in anomaly detection algorithms.\n",
    "\n",
    "### Key Bullet Points\n",
    "\n",
    "* **Synonyms:** The Gaussian distribution is also known as the **Normal Distribution** or the **bell-shaped curve**.\n",
    "* **Definition:** The probability of a random variable $x$, denoted $p(x)$, is defined by two parameters:\n",
    "    * **Mean ($\\mu$):** Determines the **center** of the bell curve.\n",
    "    * **Variance ($\\sigma^2$):** Determines the **width** or spread of the curve. $\\sigma$ is the standard deviation.\n",
    "* **Formula:** The probability density function is given by:\n",
    "    $$p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "* **Effect of Parameters:**\n",
    "    * **Increasing $\\mu$** shifts the curve horizontally (moves the center).\n",
    "    * **Increasing $\\sigma$** makes the curve wider and shorter (flatter).\n",
    "    * The total **area under the curve is always equal to 1** (a property of probability distributions).\n",
    "* **Application to Anomaly Detection (Single Feature):**\n",
    "    1.  **Estimate Parameters:** Given a training dataset of $m$ examples, the algorithm estimates $\\mu$ and $\\sigma^2$ using the **maximum likelihood estimates** (MLE).\n",
    "        * **Estimated Mean ($\\hat{\\mu}$):** The average of all training examples.\n",
    "            $$\\hat{\\mu} = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}$$\n",
    "        * **Estimated Variance ($\\hat{\\sigma}^2$):** The average squared difference from the mean.\n",
    "            $$\\hat{\\sigma}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)} - \\hat{\\mu})^2$$\n",
    "    2.  **Detection:** A new example $x_{\\text{test}}$ is considered **anomalous** if its probability $p(x_{\\text{test}})$ is very low (i.e., it falls far away in the tails of the estimated bell curve).\n",
    "* **Limitation:** This discussion only covers the case where $x$ is a single number (one feature). For practical anomaly detection with multiple features, a more advanced approach using multiple Gaussian distributions is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410a028-bfb5-4b57-b68f-d8fb036890aa",
   "metadata": {},
   "source": [
    "## Anamoly detection algorithm\n",
    "\n",
    "This section explains how to build a basic **Anomaly Detection** system using the **Gaussian Distribution** to model the probability of multiple features. This approach assumes that features are independent.\n",
    "\n",
    "### Key Bullet Points: Building the Anomaly Detection System\n",
    "\n",
    "* **Data Structure:** The training set consists of $m$ examples, where each example $\\mathbf{x}^{(i)}$ is a vector with $n$ features (e.g., $x_1$ to $x_n$).\n",
    "* **Density Estimation Model $p(\\mathbf{x})$:**\n",
    "    * The model assumes the features are **statistically independent** (a simplifying assumption that often works well in practice).\n",
    "    * The joint probability of the feature vector $\\mathbf{x}$ is the **product of the probabilities** of the individual features:\n",
    "        $$p(\\mathbf{x}) = p(x_1) \\cdot p(x_2) \\cdot \\dots \\cdot p(x_n) = \\prod_{j=1}^{n} p(x_j; \\mu_j, \\sigma_j^2)$$\n",
    "* **Algorithm Steps:**\n",
    "\n",
    "    1.  **Feature Selection:** Choose features $x_j$ that are expected to be indicative of an anomaly.\n",
    "    2.  **Parameter Fitting (Training):** Estimate the mean ($\\mu_j$) and variance ($\\sigma_j^2$) for **each feature $j$ independently** using the Maximum Likelihood Estimates (MLE) from the training data:\n",
    "        * **Mean:** $\\hat{\\mu}_j = \\frac{1}{m} \\sum_{i=1}^{m} x_j^{(i)}$ (Average of feature $j$).\n",
    "        * **Variance:** $\\hat{\\sigma}_j^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_j^{(i)} - \\hat{\\mu}_j)^2$\n",
    "    3.  **Anomaly Detection (Prediction):** For a new test example $\\mathbf{x}$, compute its probability $p(\\mathbf{x})$ using the product formula (substituting the Gaussian PDF for each $p(x_j)$).\n",
    "    4.  **Flagging:** If the computed probability **$p(\\mathbf{x})$ is less than a threshold $\\epsilon$**, flag the example as an anomaly.\n",
    "* **Intuition:** The algorithm flags an example if **one or more of its features** are unusually large or small (i.e., they fall far out in the tails of their respective Gaussian distributions), causing the overall product $p(\\mathbf{x})$ to become very small.\n",
    "* **Next Topic:** The critical next step is determining how to choose the threshold $\\epsilon$ and how to evaluate the performance of the anomaly detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622755b3-2946-4dbe-9baf-86b8b1ee6b4f",
   "metadata": {},
   "source": [
    "## Developing and evaluating an anaomoly detection system\n",
    "\n",
    "This section provides practical advice for developing and evaluating an anomaly detection system by utilizing a small set of labeled data, even though the core algorithm is unsupervised.\n",
    "\n",
    "### Key Bullet Points\n",
    "\n",
    "* **The Need for Evaluation:** Having a real-number evaluation metric is crucial for quickly making decisions (e.g., tuning $\\epsilon$, changing features) to improve the system.\n",
    "* **Creating a Labeled Dataset (for Evaluation):**\n",
    "    * While the training set remains largely unlabeled (or assumed $y=0$ for normal events), the evaluation process benefits from a small number of previously observed anomalies ($y=1$).\n",
    "    * These labeled examples are used to create **Cross-Validation (CV) and Test sets**.\n",
    "* **Data Splitting Strategy (Example: Aircraft Engines):**\n",
    "    * **Training Set (e.g., 6,000 examples):** Contains only good/normal examples ($y=0$). This is the data used to fit the Gaussian parameters ($\\mu$ and $\\sigma^2$).\n",
    "    * **Cross-Validation Set (e.g., 2,000 good + 10 anomalies):** Used to tune the threshold $\\epsilon$ and features.\n",
    "    * **Test Set (e.g., 2,000 good + 10 anomalies):** Used for final, unbiased evaluation of the system's performance.\n",
    "* **Evaluation Metrics (Handling Skewed Data):**\n",
    "    * Because the number of anomalies ($y=1$) is much smaller than the normal examples ($y=0$), the data is highly skewed.\n",
    "    * Metrics like **Precision, Recall, and the F1-Score** (which are effective for skewed data) are preferred over simple classification accuracy for evaluating performance on the CV/Test sets.\n",
    "* **Alternative Data Splitting (When Anomalies are Very Few):**\n",
    "    * If the total number of anomalies is extremely small (e.g., 2), you can forgo a separate Test set and use a Training set and a single large Cross-Validation set containing all available anomalies.\n",
    "    * **Caveat:** This approach increases the risk of **overfitting** the algorithm's parameters ($\\epsilon$ and features) to the CV set.\n",
    "* **Practical Value:** Having even a small number of labeled anomalies is very helpful for the practical process of building, tuning, and assessing an anomaly detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eac55f-12f3-4ba6-b2ef-3f3abfe3058c",
   "metadata": {},
   "source": [
    "## Anamoly detection vs. supervised learning\n",
    "\n",
    "In this section we compare Anomaly Detection and Supervised Learning, providing a framework for choosing between the two, particularly when the number of positive examples ($y=1$) is very small.\n",
    "\n",
    "### Key Bullet Points\n",
    "\n",
    "| Feature | Anomaly Detection | Supervised Learning |\n",
    "| :--- | :--- | :--- |\n",
    "| **Typical Data Balance** | Very Small number of positive examples (0–20); Large number of negative examples. | Larger number of both positive and negative examples are desired. |\n",
    "| **Model Goal** | Models normality ($y=0$); flags anything that deviates significantly. | Learns to distinguish between known classes ($y=0$ vs. $y=1$). |\n",
    "| **Handling Novelty** | Strong—Excels at detecting brand new, previously unseen types of anomalies (e.g., a new type of engine failure). | Weak—Assumes future positive examples will be similar to those in the training set. |\n",
    "| **Training Data Use** | Parameters ($\\mu, \\sigma^2$) are fit only on the negative (normal) examples. | Trained on both positive and negative examples. |\n",
    "\n",
    "### When to Choose Anomaly Detection\n",
    "\n",
    "Anomaly detection is generally preferred when:\n",
    "\n",
    "* **Positive Examples are Scarce:** You have a very small number (or zero) of known anomalies.\n",
    "* **Anomalies are Diverse/Novel:** You suspect there are many different types of anomalies, and future anomalies are likely to be unlike any observed so far (e.g., security breaches, brand new forms of financial fraud).\n",
    "* **Examples:** Detecting new types of financial fraud, monitoring machine behavior for new kinds of hacks, finding never-before-seen defects in manufacturing.\n",
    "\n",
    "### When to Choose Supervised Learning\n",
    "\n",
    "Supervised learning is preferred when:\n",
    "\n",
    "* **Sufficient Data Exists:** You have a reasonably large number of positive examples.\n",
    "* **Classes are Predictable:** Future positive examples are expected to be similar to the positive examples you've already observed (e.g., classifying known types of diseases, weather prediction).\n",
    "* **Examples:** Email spam detection (spam types are relatively consistent), detecting common, known defects in manufacturing (e.g., scratches on smartphones), or weather prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94bdf0-bc14-4476-9874-3bd519b85316",
   "metadata": {},
   "source": [
    "## Choosing what features to use\n",
    "\n",
    "This section offers practical, hands-on advice for improving the performance of an anomaly detection system, focusing heavily on feature engineering and error analysis.\n",
    "\n",
    "### Key Bullet Points: Feature Engineering for Anomaly Detection\n",
    "\n",
    "### 1. The Importance of Feature Choice\n",
    "\n",
    "* **Higher Impact than Supervised Learning:** Feature choice is more critical for anomaly detection than for supervised learning. Since the algorithm learns only from unlabeled data, it's harder for it to ignore irrelevant features or automatically rescale poor ones.\n",
    "* **Goal:** Choose features $x_j$ that take on unusually large or small values when an event is anomalous.\n",
    "\n",
    "### 2. Gaussian Transformation\n",
    "\n",
    "* **Technique:** The underlying Gaussian model works best when the features are approximately normally (Gaussian) distributed.\n",
    "* **Correction:** If a feature's histogram is highly non-Gaussian, apply a mathematical transformation to make it more Gaussian.\n",
    "    * **Common Transforms:** $\\log(x)$, $\\log(x + C)$, $\\sqrt{x}$ (or $x^{0.5}$), $x^c$, etc.\n",
    "    * **Process:** Plot a histogram of the transformed feature and visually select the transform that yields the best bell-shaped curve.\n",
    "    * **Rule:** Apply the same transformation to the training, cross-validation, and test sets.\n",
    "\n",
    "### 3. Error Analysis and Feature Creation\n",
    "\n",
    "* **Process:** If the system performs poorly on the cross-validation set (fails to detect known anomalies), perform an **error analysis**:\n",
    "    1.  **Examine Missed Anomalies:** Look at the specific anomalous examples ($\\mathbf{x}_{\\text{anom}}$) the algorithm failed to flag (i.e., $p(\\mathbf{x}_{\\text{anom}}) \\ge \\epsilon$).\n",
    "    2.  **Identify New Features:** Determine what characteristics of that missed anomaly made you, the human expert, think it was anomalous.\n",
    "    3.  **Create New Features:** Add new features that capture that specific unusual characteristic, often by combining existing features.\n",
    "* **Feature Combination Examples:**\n",
    "    * If two features ($x_1$ and $x_2$) are individually normal but unusual when combined, create a new ratio feature: $x_5 = x_3 / x_4$ (e.g., CPU load to network traffic ratio).\n",
    "    * This makes it easier for the $p(\\mathbf{x})$ model to assign a very low probability to that combination, successfully flagging the anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba656ce-22ed-4bac-a0b8-933dd6b10d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
