{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d221adb0-3aa9-46f9-a083-ef7df01fbb7d",
   "metadata": {},
   "source": [
    "# Week 2: Recommender Systems\n",
    "---\n",
    "## Table of Contents\n",
    "* [Making Recommendations](#Making-Recommendations)\n",
    "* [Using per-item features](#Using-per-item-features)\n",
    "* [Collaborative filtering algorithm](#Collaborative-filtering-algorithm)\n",
    "* [Binary labels: favs, likes and clicks](#Binary-labels:-favs,-likes-and-clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d2506-955d-4f91-bd5f-19cfd3a2a2bd",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad6020-87f3-4c9d-8e98-18d98708a993",
   "metadata": {},
   "source": [
    "## Making Recommendations\n",
    "\n",
    "This section introduces the topic of Recommender Systems, highlighting their significant commercial impact and setting up the basic framework and notation using the example of movie rating prediction.\n",
    "\n",
    "---\n",
    "### Commercial Importance\n",
    "* **Widespread Use:** Recommender systems are used everywhere online (e.g., shopping sites like Amazon, streaming services like Netflix, food delivery apps).\n",
    "* **High Value:** For many companies, a large fraction of sales and economic value is directly driven by the success of their recommender systems.\n",
    "* **Academic vs. Commercial Attention:** The commercial impact of recommender systems is arguably vastly greater than the attention it receives in academia.\n",
    "\n",
    "### Core Framework (Movie Rating Example)\n",
    "The goal is to predict how users would rate movies they haven't yet watched (denoted by '?') to decide what to recommend.\n",
    "\n",
    "| Item | Notation | Definition/Example |\n",
    "| :--- | :--- | :--- |\n",
    "| **Number of Users** | $n_u$ | In the example, $n_u = 4$ (Alice, Bob, Carol, Dave). |\n",
    "| **Number of Items (Movies)** | $n_m$ | In the example, $n_m = 5$. |\n",
    "| **Rating Indicator** | $r(i, j)$ | A binary value: $r(i, j) = 1$ if user $j$ has rated movie $i$; $0$ otherwise. |\n",
    "| **Actual Rating** | $y^{(i, j)}$ | The rating (0 to 5 stars) given by user $j$ to movie $i$. (E.g., $y^{(3, 2)} = 4$). |\n",
    "\n",
    "### Next Step\n",
    "* The subsequent lesson will begin developing an algorithm to predict the missing ratings. The first model will temporarily assume that **features (extra information)** about the movies (e.g., whether it is a romance movie or an action movie) are already available. Later in the notes, we will address how to build the system when these explicit movie features are not available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27fe117-e203-4d24-951e-5556e3a65fd6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dcb8a0-978b-4734-b356-2b2064f41b98",
   "metadata": {},
   "source": [
    "## Using per-item features\n",
    "\n",
    "This section details the first approach to building a recommender system: **using pre-existing item features** to create a personalized linear regression model for each user.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Bullet Points: Recommender Systems with Item Features\n",
    "\n",
    "#### Framework and Notation\n",
    "* **Initial Assumption:** We have pre-defined **features ($X$)** for each item (movie), such as $x_1$ (Romance level) and $x_2$ (Action level).\n",
    "    * $n_u$: Number of users (e.g., 4).\n",
    "    * $n_m$: Number of movies/items (e.g., 5).\n",
    "    * $n$: Number of features (e.g., 2).\n",
    "    * $r(i, j) = 1$: User $j$ has rated movie $i$.\n",
    "    * $y^{(i, j)}$: The actual rating given by user $j$ to movie $i$.\n",
    "\n",
    "#### The Model: Personalized Linear Regression\n",
    "* The system fits a **separate linear regression model for each user $j$** to predict their rating for any movie $i$.\n",
    "* **Prediction Formula:**\n",
    "    $$\\text{Prediction for } y^{(i, j)} = \\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)}$$\n",
    "    * $\\mathbf{w}^{(j)}$ and $b^{(j)}$ are the unique parameters (weights and bias) learned for **user $j$**.\n",
    "    * $\\mathbf{x}^{(i)}$ is the feature vector for **movie $i$**.\n",
    "\n",
    "### The Cost Function\n",
    "The objective is to learn the parameters ($\\mathbf{w}^{(j)}$ and $b^{(j)}$) for **all users** simultaneously by minimizing a regularized mean squared error cost function.\n",
    "\n",
    "* **Cost Function for All Users ($J$):** The cost is the sum of the individual cost functions for every user.\n",
    "    $$J(\\mathbf{w}^{(1)}, b^{(1)}, \\dots, \\mathbf{w}^{(n_u)}, b^{(n_u)}) = \\sum_{j=1}^{n_u} J(\\mathbf{w}^{(j)}, b^{(j)})$$\n",
    "\n",
    "* **Individual User Cost ($J(\\mathbf{w}^{(j)}, b^{(j)})$):**\n",
    "    $$J(\\mathbf{w}^{(j)}, b^{(j)}) = \\frac{1}{2} \\sum_{i: r(i, j)=1} \\left( (\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)}) - y^{(i, j)} \\right)^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{n} (w_k^{(j)})^2$$\n",
    "    * The sum $\\sum_{i: r(i, j)=1}$ means we only calculate the error for movies that **user $j$ has actually rated**.\n",
    "    * The second term is standard **regularization** to prevent overfitting. (Note: The normalization constant $1/m^{(j)}$ is omitted for convenience, as it doesn't change the parameters at the minimum; see Bonus section below).\n",
    "\n",
    "### Next Challenge\n",
    "* The current method relies on having **pre-defined features ($\\mathbf{x}^{(i)}$)** for every item.\n",
    "* The next section will explore a modification of this algorithm—**Collaborative Filtering**—which works even when these detailed item features are **not available** beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b379da-5907-4ff3-8e6d-965ff6c4a3ee",
   "metadata": {},
   "source": [
    "## Bonus: Dropping $m^{(j)}$ and model performance\n",
    "The reason that dropping the term related to the number of movies rated by user $j$, denoted here as $m^{(j)}$ (or the number of training examples), from the denominator of the cost function **does not affect the model performance** is rooted in how optimization works.\n",
    "\n",
    "---\n",
    "\n",
    "### The Role of the Denominator in Optimization\n",
    "\n",
    "The individual cost function for user $j$ (before dropping $m^{(j)}$) typically looks like this:\n",
    "\n",
    "$$J(\\mathbf{w}^{(j)}, b^{(j)}) = \\frac{1}{2m^{(j)}} \\sum_{i: r(i, j)=1} \\left( \\text{Prediction} - \\text{Actual Rating} \\right)^2 + \\frac{\\lambda}{2m^{(j)}} \\sum_{k=1}^{n} (w_k^{(j)})^2$$\n",
    "\n",
    "### 1. It's Just a Scaling Constant\n",
    "\n",
    "When optimizing the model, the goal is to find the set of parameters ($\\mathbf{w}^{(j)}$ and $b^{(j)}$) that **minimize** the value of $J$.\n",
    "\n",
    "* In this context, $\\frac{1}{2m^{(j)}}$ is a **constant scaling factor**. It is a fixed number determined by the training data *before* the optimization process begins.\n",
    "* Multiplying or dividing the entire cost function by a positive constant only **scales the cost function vertically**; it **does not change the location** of the minimum point.\n",
    "\n",
    "### 2. The Minimum Remains the Same\n",
    "\n",
    "Imagine a simple parabolic function, $f(x) = x^2$. The minimum occurs at $x=0$.\n",
    "If you scale it by a constant $c=5$, the new function is $g(x) = 5x^2$. The minimum still occurs at **$x=0$**.\n",
    "\n",
    "In the recommender system:\n",
    "\n",
    "$$\\text{arg min}_{\\mathbf{w}, b} \\left[ J_{\\text{original}}(\\mathbf{w}, b) \\right] = \\text{arg min}_{\\mathbf{w}, b} \\left[ \\mathbf{C} \\cdot J_{\\text{simplified}}(\\mathbf{w}, b) \\right]$$\n",
    "\n",
    "Where $\\mathbf{C} = \\frac{1}{2m^{(j)}}$ is the constant.\n",
    "\n",
    "Since both the squared error term and the regularization term are multiplied by the same constant, the parameter values ($\\mathbf{w}^{(j)}$ and $b^{(j)}$) that make the original function minimum are **the exact same values** that make the simplified function minimum.\n",
    "\n",
    "### 3. Impact on Gradient Descent\n",
    "\n",
    "This simplification is also beneficial when using Gradient Descent:\n",
    "\n",
    "* The gradients (derivatives) of the cost function are used to determine the step size and direction.\n",
    "* Dropping the constant $\\frac{1}{2m^{(j)}}$ just **scales the magnitude of the gradient**. We compensate for this by simply adjusting the **learning rate ($\\alpha$)** used in Gradient Descent. If the cost function is scaled up, we just use a smaller learning rate, and vice versa.\n",
    "\n",
    "In summary, the normalization constant is often included in academic settings for statistical correctness (to compute an average loss), but for the purely practical goal of finding the optimal parameters, it can be safely dropped to simplify the mathematical expression.\n",
    "\n",
    "### How is this different from cost function in linear or logistic regression?\n",
    "\n",
    "While you *could* technically drop the division by $m$ (the number of training examples) in the denominator of the standard Linear Regression cost function, it is almost always kept.\n",
    "\n",
    "Here is why $1/m$ (or $1/2m$) is typically included in **Linear Regression** but often dropped in **Recommender Systems** (like the individual user cost):\n",
    "\n",
    "---\n",
    "\n",
    "### Why the $1/m$ Term is Kept in Linear Regression\n",
    "\n",
    "The standard cost function for linear regression (Mean Squared Error, MSE) is:\n",
    "\n",
    "$$J(\\mathbf{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( (\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - y^{(i)} \\right)^2$$\n",
    "\n",
    "### 1. Statistical Meaning (Averaging)\n",
    "The most important reason is to define the cost as the **average loss per example**.\n",
    "\n",
    "* **Interpretability:** By dividing by $m$, the value of $J(\\mathbf{w}, b)$ becomes the **average squared error**. This gives the cost function an intuitive meaning: if you add more data, the cost value doesn't automatically skyrocket; it represents the error regardless of the dataset size.\n",
    "* **Comparison:** It allows you to **compare models and performance across different datasets** of varying sizes. A model trained on 100 examples with an MSE of 5 is directly comparable to a model trained on 1,000 examples with an MSE of 5.\n",
    "\n",
    "### 2. Standard Practice and Consistency\n",
    "The MSE is the standard, well-established error metric. Keeping the division by $m$ aligns with textbook definitions and ensures that the final reported loss value is the actual MSE, which is important for evaluation and publication.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why the $1/m^{(j)}$ Term is Dropped in Recommender Systems\n",
    "\n",
    "In the individual user cost for the recommender system, the term $\\frac{1}{2m^{(j)}}$ is often simplified to just $\\frac{1}{2}$ (or dropped entirely) for two main reasons:\n",
    "\n",
    "### 1. Simplification for Joint Optimization\n",
    "In the recommender system, the overall cost function $J_{\\text{overall}}$ is the **sum of all individual user costs** $J(\\mathbf{w}^{(j)}, b^{(j)})$:\n",
    "\n",
    "$$J_{\\text{overall}} = \\sum_{j=1}^{n_u} \\left( \\frac{1}{2} \\sum_{i: r(i, j)=1} (\\dots)^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{n} (w_k^{(j)})^2 \\right)$$\n",
    "\n",
    "When calculating the gradient for the whole system, having different division factors ($m^{(j)}$) for every single user's loss term and every single user's regularization term makes the algebra unnecessarily complex. Dropping the $m^{(j)}$ allows for a cleaner, unified cost function for the entire system, where we are primarily concerned with **minimization**, not calculating a statistically precise average.\n",
    "\n",
    "### 2. Relative Size of Regularization\n",
    "In recommender systems, the relationship between the regularization term ($\\lambda$) and the error term is crucial.\n",
    "\n",
    "If you keep the $1/m^{(j)}$ in the error term, you must also decide whether to keep it in the regularization term. If the goal is simplification without affecting the minimum, removing the constant scaling factor from the entire loss calculation (including regularization) is the cleanest route.\n",
    "\n",
    "---\n",
    "\n",
    "In essence:\n",
    "\n",
    "| Context | Purpose of $J$ | Why $1/m$ is Kept/Dropped |\n",
    "| :--- | :--- | :--- |\n",
    "| **Linear Regression** | **Evaluation and Comparison (MSE)** | Kept, because it defines the **average error (MSE)**, which is the standard, comparable metric. |\n",
    "| **Recommender System** | **Optimization** | Dropped, because $m^{(j)}$ is a **constant** that unnecessarily complicates the overall cost function when summing across users, and it doesn't change the optimal parameter values. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e82ca-5576-4228-b9dc-f89b10aed2aa",
   "metadata": {},
   "source": [
    "## Collaborative filtering algorithm\n",
    "\n",
    "This section introduces **Collaborative Filtering**, a powerful technique for recommender systems where the item features ($\\mathbf{x}$) are **learned from the user ratings** rather than being provided in advance.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Bullet Points: Collaborative Filtering Algorithm\n",
    "\n",
    "#### 1. The Challenge: Learning Item Features ($\\mathbf{x}$)\n",
    "* **Previous Model:** Assumed movie features ($\\mathbf{x}$) were known (e.g., Romance, Action level).\n",
    "* **New Approach:** When features are unknown, the ratings provided by **multiple users** on the same item can be leveraged to learn what those item features ($\\mathbf{x}$) should be.\n",
    "    * **Why it Works:** Having ratings from several users (each with known preference parameters $\\mathbf{w}$ and $b$) allows the system to infer the features of an unfeatured movie that best explain those ratings.\n",
    "    * **Collaborative Filtering Name:** This relies on the \"collaboration\" of ratings from multiple users on the same item.\n",
    "\n",
    "#### 2. Cost Function for Learning Features ($\\mathbf{x}$)\n",
    "If the user preference parameters ($\\mathbf{w}^{(j)}, b^{(j)}$) are temporarily fixed, the features for a single movie $i$ ($\\mathbf{x}^{(i)}$) are learned by minimizing the cost function:\n",
    "\n",
    "$$\\min_{\\mathbf{x}^{(i)}} J(\\mathbf{x}^{(i)}) = \\frac{1}{2} \\sum_{j: r(i, j)=1} \\left( (\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)}) - y^{(i, j)} \\right)^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{n} (x_k^{(i)})^2$$\n",
    "\n",
    "#### 3. The Full Collaborative Filtering Cost Function\n",
    "The final algorithm combines the objective of learning user preferences ($\\mathbf{w}, b$) and learning item features ($\\mathbf{x}$) into a single unified cost function ($J$):\n",
    "\n",
    "* **Minimization:** The algorithm simultaneously minimizes $J$ with respect to all parameters: the user parameters ($\\mathbf{w}^{(j)}, b^{(j)}$ for all users $j$) and the movie features ($\\mathbf{x}^{(i)}$ for all movies $i$).\n",
    "* **Unified Cost ($J$):** This combines the prediction error and the regularization terms for both users and movies.\n",
    "\n",
    "$$J(\\mathbf{w}, \\mathbf{b}, \\mathbf{x}) = \\frac{1}{2} \\sum_{(i, j): r(i, j)=1} \\left( (\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)}) - y^{(i, j)} \\right)^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^{n} (w_k^{(j)})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m} \\sum_{k=1}^{n} (x_k^{(i)})^2$$\n",
    "\n",
    "#### 4. Optimization\n",
    "* **Method:** Gradient Descent or other optimization algorithms are used to minimize the cost function $J$.\n",
    "* **Parameters:** In this full formulation, both the user preferences ($\\mathbf{w}, \\mathbf{b}$) and the item features ($\\mathbf{x}$) are treated as parameters to be learned and are updated iteratively.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_i^{(j)} &= w_i^{(j)} - \\alpha \\frac{\\partial}{\\partial w_i^{(j)}}J(w, b,x) \\\\\n",
    "b^{(j)} &= b^{(j)} - \\alpha \\frac{\\partial}{\\partial b^{(j)}}J(w, b,x) \\\\\n",
    "x_k^{(i)} &= x_k^{(i)} - \\alpha \\frac{\\partial}{\\partial x_k^{(i)}}J(w, b,x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Next Step\n",
    "The next discussion will address a generalization of this model to systems using **binary labels** (e.g., like/dislike) instead of continuous star ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac93af2-037c-497f-95bf-851c0662f6f9",
   "metadata": {},
   "source": [
    "## Binary labels: favs, likes and clicks\n",
    "\n",
    "This section explains how to adapt the collaborative filtering algorithm from predicting continuous ratings (like 1–5 stars) to predicting **binary labels** (like/dislike, purchase/not purchase), using a method analogous to moving from linear regression to logistic regression.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Bullet Points: Collaborative Filtering with Binary Labels\n",
    "\n",
    "#### 1. Binary Label Context\n",
    "* **Problem:** Many recommender systems deal with binary labels (1 or 0) rather than star ratings.\n",
    "* **Label Meanings (Engagement):**\n",
    "    * **1 (Engaged):** User liked, purchased, favorited, clicked, or spent a minimum time (e.g., 30 seconds) on an item after exposure.\n",
    "    * **0 (Not Engaged):** User did not like, did not purchase, or left quickly after being exposed to the item.\n",
    "    * **? (Question Mark):** The user was not yet exposed to the item (no rating/engagement data).\n",
    "* **Goal:** Predict the probability that a user will like or engage with a new item (the '?' items) to decide what to recommend.\n",
    "\n",
    "#### 2. The Model: Logistic Regression Analogy\n",
    "* **Prediction Shift:** The model shifts from predicting a numerical rating to predicting a probability of engagement.\n",
    "* **Logistic Function:** The linear combination of user preferences ($\\mathbf{w}^{(j)}$) and item features ($\\mathbf{x}^{(i)}$) is passed through the logistic function ($g$) (also known as the sigmoid function).\n",
    "    * **Probability Prediction:**\n",
    "    $$\\text{P}(y^{(i, j)}=1) = g(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)})$$\n",
    "    * Where $g(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "#### 3. The Cost Function: Binary Cross-Entropy\n",
    "* **Cost Function Modification:** Since the output is a probability and the labels are binary, the squared error cost function (used for ratings) is replaced with the Binary Cross-Entropy Loss (or log loss), which is standard for logistic regression.\n",
    "* **Loss for a Single Example:**\n",
    "    $$L(f(\\mathbf{x}), y) = -y \\log(f(\\mathbf{x})) - (1-y) \\log(1-f(\\mathbf{x}))$$\n",
    "* **Overall Binary Collaborative Filtering Cost ($J$):** The total cost function sums this binary cross-entropy loss over all user-item pairs where a rating/engagement exists ($r^{(i, j)}=1$), plus the regularization terms for all $\\mathbf{w}$, $\\mathbf{b}$, and $\\mathbf{x}$.\n",
    "\n",
    "#### 4. Generalization\n",
    "* This generalization significantly opens up the set of applications that can be addressed by collaborative filtering, allowing the algorithm to work with implicit feedback (like clicks or viewing time) rather than requiring explicit user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7752d-48ad-4241-93ca-b14d315acbed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DS)",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
