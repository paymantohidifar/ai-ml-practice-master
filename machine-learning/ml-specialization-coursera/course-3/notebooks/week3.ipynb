{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f497b5c8-f4f6-48ce-94f9-4628f49effed",
   "metadata": {},
   "source": [
    "# Week 3: Reinforcement Learning\n",
    "\n",
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b885f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f789f",
   "metadata": {},
   "source": [
    "## What is reinforcement learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a fundamental pillar of machine learning used to train an **agent** (like a robot or an algorithm) to make a sequence of decisions in an environment by maximizing a cumulative **reward**.\n",
    "\n",
    "Instead of relying on labeled data (like supervised learning), RL uses a system of trial and error guided by a reward function.\n",
    "\n",
    "### Core Concepts and Mechanism\n",
    "\n",
    "* **Goal:** To find a function (often called a **policy**) that maps an observed **State** ($\\mathbf{s}$) of the environment to an optimal **Action** ($\\mathbf{a}$).\n",
    "* **State ($\\mathbf{s}$):** The agent's current situation or observation (e.g., a helicopter's position, orientation, and speed).\n",
    "* **Action ($\\mathbf{a}$):** A decision the agent makes (e.g., how to move the control sticks).\n",
    "* **Reward Function:** The key input to RL, which tells the agent *when* it is doing well and *when* it is doing poorly.\n",
    "    * **Incentive:** The agent's task is to figure out the sequence of actions that maximize the total cumulative reward over time.\n",
    "    * **Example:** For a helicopter, reward may be **+1** for every second flying well and a large **negative reward** (e.g., -1000) for crashing. \n",
    "\n",
    "<img src='images/rl.png' width='500px'>\n",
    "\n",
    "### The Power of Reward\n",
    "\n",
    "RL is powerful because the designer only needs to specify **what** the goal is (via the reward function), not **how** to achieve it (via specific optimal actions).\n",
    "* **Analogy:** It is like training a dog; you reward \"good dog\" behavior and discourage \"bad dog\" behavior, allowing the dog to learn the complex path to the desired outcome itself.\n",
    "* **Example:** An RL algorithm enabled a robot dog to learn complex leg placements to climb over obstacles solely by rewarding progress toward the goal, without explicit instructions on leg movement.\n",
    "\n",
    "### Contrast with Supervised Learning (SL)\n",
    "\n",
    "For many control tasks (like flying a robot), Supervised Learning (SL) fails because it requires a large dataset of states ($\\mathbf{x}$) and their ideal actions ($\\mathbf{y}$).\n",
    "\n",
    "It is often ambiguous or impossible for a human expert to define the single, exact \"right action\" ($\\mathbf{y}$) for every single complex state ($\\mathbf{x}$), making SL impractical for these scenarios. RL overcomes this ambiguity by using rewards instead of perfect labels.\n",
    "\n",
    "### Applications\n",
    "\n",
    "* **Robotics:** Controlling autonomous systems (helicopters, drones, robot dogs) to perform complex maneuvers.\n",
    "* **Optimization:** Factory optimization to maximize throughput and efficiency.\n",
    "* **Finance:** Efficient stock execution and trading strategies (e.g., sequencing trades to minimize price impact).\n",
    "* **Gaming:** Playing complex games like Chess, Go, Bridge, and various video games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bda744",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686110fa",
   "metadata": {},
   "source": [
    "## Mars rover example\n",
    "\n",
    "This section formalizes the core concepts of Reinforcement Learning (RL) using a simplified example inspired by the Mars rover, introducing the concepts of state, action, reward, and terminal states.\n",
    "\n",
    "### The Environment Setup (States and Rewards)\n",
    "\n",
    "* **States ($S$):** The environment is modeled as a sequence of six positions, $S_1$ through $S_6$, representing possible locations of the rover. The rover starts in $S_4$.\n",
    "* **Rewards ($R$):** Rewards are associated with specific states based on their scientific value:\n",
    "    * $R(S_1) = 100$ (Highest value, most interesting science).\n",
    "    * $R(S_6) = 40$ (Second highest value).\n",
    "    * $R(S_2) = R(S_3) = R(S_4) = R(S_5) = 0$.\n",
    "* **Terminal States:** $S_1$ and $S_6$ are terminal states. Once the rover reaches these states, the day (or episode) ends, and no further rewards can be earned.\n",
    "\n",
    "### Actions and Transitions\n",
    "\n",
    "* **Actions ($A$):** At each step, the rover can choose one of two actions:\n",
    "    * Go Left\n",
    "    * Go Right\n",
    "* **State Transition:** Taking an action leads the rover from the current state $S$ to a new state $S'$ (the next state). For example, from $S_4$, taking the action \"Go Left\" leads to the next state $S_3$.\n",
    "\n",
    "<img src='images/rl_example.png' width=600px>\n",
    "\n",
    "### The Core RL Loop Elements\n",
    "\n",
    "The fundamental process that defines the reinforcement learning problem is the sequence of transitions:\n",
    "\n",
    "At every time step, the robot is in a State ($\\mathbf{S}$), chooses an Action ($\\mathbf{A}$), receives the Reward ($\\mathbf{R}(S)$) associated with that state, and transitions to a Next State ($\\mathbf{S'}$).\n",
    "\n",
    "### Evaluating Action Sequences\n",
    "\n",
    "The goal of the RL algorithm is to figure out the optimal sequence of actions to maximize the total reward collected before reaching a terminal state.\n",
    "\n",
    "* **Option 1 (Go Left):** $S_4 \\to S_3 \\to S_2 \\to S_1$. Total Reward: $0 + 0 + 0 + 100 = 100$.\n",
    "* **Option 2 (Go Right):** $S_4 \\to S_5 \\to S_6$. Total Reward: $0 + 0 + 40 = 40$.\n",
    "* **Suboptimal Path:** $S_4 \\to S_5 \\to S_4 \\to \\dots$ (Wasting time by moving back and forth).\n",
    "\n",
    "The algorithm must learn to choose the path (policy) that yields the highest cumulative return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f54d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246384e",
   "metadata": {},
   "source": [
    "## The return in reinforcement learning\n",
    "\n",
    "The section explains the critical Reinforcement Learning (RL) concept of the **Return**, which is used to evaluate the desirability of a sequence of rewards by introducing a **Discount Factor ($\\gamma$)**.\n",
    "\n",
    "### Defining the Return ($G$)\n",
    "\n",
    "The Return is the single number used to quantify the total cumulative value of a sequence of rewards ($R_1, R_2, R_3, \\dots$) that an agent receives over an episode.\n",
    "* **Analogy:** It captures the idea that a smaller, immediate reward (like a \\$5 bill now) might be more attractive than a larger reward that takes significant time and effort to obtain (like a \\$10 bill across town).\n",
    "* **Goal:** The primary objective of an RL algorithm is to find a policy (set of actions) that maximizes the expected Return.\n",
    "\n",
    "### The Discount Factor ($\\gamma$)\n",
    "\n",
    "The discount factor, $\\gamma$ (gamma), is a number between 0 and 1 (often close to 1, like 0.9 or 0.99) used to weigh future rewards. It makes the RL algorithm **\"impatient\"** by reducing the value of rewards received later in time. Rewards received sooner contribute more to the total Return.\n",
    "\n",
    "### The Return Formula\n",
    "\n",
    "The return ($G$) is calculated as the sum of all future rewards, where each successive reward is discounted by an increasing power of $\\gamma$:\n",
    "\n",
    "$$G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\dots$$\n",
    "\n",
    "* The first reward ($R_1$) is given full credit ($1 \\cdot R_1$).\n",
    "* The second reward ($R_2$) is multiplied by $\\gamma$.\n",
    "* The third reward ($R_3$) is multiplied by $\\gamma^2$, and so on.\n",
    "\n",
    "### Interpretation and Practical Effects\n",
    "\n",
    "* **Financial Interpretation:** In applications like financial trading, $\\gamma$ often represents the time value of money or the interest rate, meaning a dollar today is worth more than a dollar in the future.\n",
    "* **Handling Negative Rewards:** If the system incurs negative rewards (costs or penalties), the discount factor incentivizes the algorithm to push these negative outcomes as far into the future as possible, minimizing their discounted impact on the total Return.\n",
    "* **Policy Dependence:** The Return obtained from any given state depends entirely on the actions (policy) the agent chooses. In the rover example, choosing \"Go Left\" yielded a higher Return (12.5) than choosing \"Go Right\" (10) from the starting state $S_4$.\n",
    "\n",
    "<img src='images/rl_return.png' width='700px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5accbf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
