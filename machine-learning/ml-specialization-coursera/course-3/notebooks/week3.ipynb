{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f497b5c8-f4f6-48ce-94f9-4628f49effed",
   "metadata": {},
   "source": [
    "# Week 3: Reinforcement Learning\n",
    "\n",
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b885f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f789f",
   "metadata": {},
   "source": [
    "## What is reinforcement learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a fundamental pillar of machine learning used to train an **agent** (like a robot or an algorithm) to make a sequence of decisions in an environment by maximizing a cumulative **reward**.\n",
    "\n",
    "Instead of relying on labeled data (like supervised learning), RL uses a system of trial and error guided by a reward function.\n",
    "\n",
    "### Core Concepts and Mechanism\n",
    "\n",
    "* **Goal:** To find a function (often called a **policy**) that maps an observed **State** ($\\mathbf{s}$) of the environment to an optimal **Action** ($\\mathbf{a}$).\n",
    "* **State ($\\mathbf{s}$):** The agent's current situation or observation (e.g., a helicopter's position, orientation, and speed).\n",
    "* **Action ($\\mathbf{a}$):** A decision the agent makes (e.g., how to move the control sticks).\n",
    "* **Reward Function:** The key input to RL, which tells the agent *when* it is doing well and *when* it is doing poorly.\n",
    "    * **Incentive:** The agent's task is to figure out the sequence of actions that maximize the total cumulative reward over time.\n",
    "    * **Example:** For a helicopter, reward may be **+1** for every second flying well and a large **negative reward** (e.g., -1000) for crashing. \n",
    "\n",
    "<img src='images/rl.png' width='500px'>\n",
    "\n",
    "### The Power of Reward\n",
    "\n",
    "RL is powerful because the designer only needs to specify **what** the goal is (via the reward function), not **how** to achieve it (via specific optimal actions).\n",
    "* **Analogy:** It is like training a dog; you reward \"good dog\" behavior and discourage \"bad dog\" behavior, allowing the dog to learn the complex path to the desired outcome itself.\n",
    "* **Example:** An RL algorithm enabled a robot dog to learn complex leg placements to climb over obstacles solely by rewarding progress toward the goal, without explicit instructions on leg movement.\n",
    "\n",
    "### Contrast with Supervised Learning (SL)\n",
    "\n",
    "For many control tasks (like flying a robot), Supervised Learning (SL) fails because it requires a large dataset of states ($\\mathbf{x}$) and their ideal actions ($\\mathbf{y}$).\n",
    "\n",
    "It is often ambiguous or impossible for a human expert to define the single, exact \"right action\" ($\\mathbf{y}$) for every single complex state ($\\mathbf{x}$), making SL impractical for these scenarios. RL overcomes this ambiguity by using rewards instead of perfect labels.\n",
    "\n",
    "### Applications\n",
    "\n",
    "* **Robotics:** Controlling autonomous systems (helicopters, drones, robot dogs) to perform complex maneuvers.\n",
    "* **Optimization:** Factory optimization to maximize throughput and efficiency.\n",
    "* **Finance:** Efficient stock execution and trading strategies (e.g., sequencing trades to minimize price impact).\n",
    "* **Gaming:** Playing complex games like Chess, Go, Bridge, and various video games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bda744",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
