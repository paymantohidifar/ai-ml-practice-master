{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f497b5c8-f4f6-48ce-94f9-4628f49effed",
   "metadata": {},
   "source": [
    "# Week 3: Reinforcement Learning\n",
    "\n",
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b885f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f789f",
   "metadata": {},
   "source": [
    "## What is reinforcement learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a fundamental pillar of machine learning used to train an **agent** (like a robot or an algorithm) to make a sequence of decisions in an environment by maximizing a cumulative **reward**.\n",
    "\n",
    "Instead of relying on labeled data (like supervised learning), RL uses a system of trial and error guided by a reward function.\n",
    "\n",
    "### Core Concepts and Mechanism\n",
    "\n",
    "* **Goal:** To find a function (often called a **policy**) that maps an observed **State** ($\\mathbf{s}$) of the environment to an optimal **Action** ($\\mathbf{a}$).\n",
    "* **State ($\\mathbf{s}$):** The agent's current situation or observation (e.g., a helicopter's position, orientation, and speed).\n",
    "* **Action ($\\mathbf{a}$):** A decision the agent makes (e.g., how to move the control sticks).\n",
    "* **Reward Function:** The key input to RL, which tells the agent *when* it is doing well and *when* it is doing poorly.\n",
    "    * **Incentive:** The agent's task is to figure out the sequence of actions that maximize the total cumulative reward over time.\n",
    "    * **Example:** For a helicopter, reward may be **+1** for every second flying well and a large **negative reward** (e.g., -1000) for crashing. \n",
    "\n",
    "<img src='images/rl.png' width='500px'>\n",
    "\n",
    "### The Power of Reward\n",
    "\n",
    "RL is powerful because the designer only needs to specify **what** the goal is (via the reward function), not **how** to achieve it (via specific optimal actions).\n",
    "* **Analogy:** It is like training a dog; you reward \"good dog\" behavior and discourage \"bad dog\" behavior, allowing the dog to learn the complex path to the desired outcome itself.\n",
    "* **Example:** An RL algorithm enabled a robot dog to learn complex leg placements to climb over obstacles solely by rewarding progress toward the goal, without explicit instructions on leg movement.\n",
    "\n",
    "### Contrast with Supervised Learning (SL)\n",
    "\n",
    "For many control tasks (like flying a robot), Supervised Learning (SL) fails because it requires a large dataset of states ($\\mathbf{x}$) and their ideal actions ($\\mathbf{y}$).\n",
    "\n",
    "It is often ambiguous or impossible for a human expert to define the single, exact \"right action\" ($\\mathbf{y}$) for every single complex state ($\\mathbf{x}$), making SL impractical for these scenarios. RL overcomes this ambiguity by using rewards instead of perfect labels.\n",
    "\n",
    "### Applications\n",
    "\n",
    "* **Robotics:** Controlling autonomous systems (helicopters, drones, robot dogs) to perform complex maneuvers.\n",
    "* **Optimization:** Factory optimization to maximize throughput and efficiency.\n",
    "* **Finance:** Efficient stock execution and trading strategies (e.g., sequencing trades to minimize price impact).\n",
    "* **Gaming:** Playing complex games like Chess, Go, Bridge, and various video games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bda744",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686110fa",
   "metadata": {},
   "source": [
    "## Mars rover example\n",
    "\n",
    "This section formalizes the core concepts of Reinforcement Learning (RL) using a simplified example inspired by the Mars rover, introducing the concepts of state, action, reward, and terminal states.\n",
    "\n",
    "### The Environment Setup (States and Rewards)\n",
    "\n",
    "* **States ($S$):** The environment is modeled as a sequence of six positions, $S_1$ through $S_6$, representing possible locations of the rover. The rover starts in $S_4$.\n",
    "* **Rewards ($R$):** Rewards are associated with specific states based on their scientific value:\n",
    "    * $R(S_1) = 100$ (Highest value, most interesting science).\n",
    "    * $R(S_6) = 40$ (Second highest value).\n",
    "    * $R(S_2) = R(S_3) = R(S_4) = R(S_5) = 0$.\n",
    "* **Terminal States:** $S_1$ and $S_6$ are terminal states. Once the rover reaches these states, the day (or episode) ends, and no further rewards can be earned.\n",
    "\n",
    "### Actions and Transitions\n",
    "\n",
    "* **Actions ($A$):** At each step, the rover can choose one of two actions:\n",
    "    * Go Left\n",
    "    * Go Right\n",
    "* **State Transition:** Taking an action leads the rover from the current state $S$ to a new state $S'$ (the next state). For example, from $S_4$, taking the action \"Go Left\" leads to the next state $S_3$.\n",
    "\n",
    "<img src='images/rl_example.png' width=600px>\n",
    "\n",
    "### The Core RL Loop Elements\n",
    "\n",
    "The fundamental process that defines the reinforcement learning problem is the sequence of transitions:\n",
    "\n",
    "At every time step, the robot is in a State ($\\mathbf{S}$), chooses an Action ($\\mathbf{A}$), receives the Reward ($\\mathbf{R}(S)$) associated with that state, and transitions to a Next State ($\\mathbf{S'}$).\n",
    "\n",
    "### Evaluating Action Sequences\n",
    "\n",
    "The goal of the RL algorithm is to figure out the optimal sequence of actions to maximize the total reward collected before reaching a terminal state.\n",
    "\n",
    "* **Option 1 (Go Left):** $S_4 \\to S_3 \\to S_2 \\to S_1$. Total Reward: $0 + 0 + 0 + 100 = 100$.\n",
    "* **Option 2 (Go Right):** $S_4 \\to S_5 \\to S_6$. Total Reward: $0 + 0 + 40 = 40$.\n",
    "* **Suboptimal Path:** $S_4 \\to S_5 \\to S_4 \\to \\dots$ (Wasting time by moving back and forth).\n",
    "\n",
    "The algorithm must learn to choose the path (policy) that yields the highest cumulative return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f54d1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
