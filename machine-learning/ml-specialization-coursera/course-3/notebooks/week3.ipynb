{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f497b5c8-f4f6-48ce-94f9-4628f49effed",
   "metadata": {},
   "source": [
    "# Week 3: Reinforcement Learning\n",
    "\n",
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b885f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f789f",
   "metadata": {},
   "source": [
    "## What is reinforcement learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a fundamental pillar of machine learning used to train an **agent** (like a robot or an algorithm) to make a sequence of decisions in an environment by maximizing a cumulative **reward**.\n",
    "\n",
    "Instead of relying on labeled data (like supervised learning), RL uses a system of trial and error guided by a reward function.\n",
    "\n",
    "### Core Concepts and Mechanism\n",
    "\n",
    "* **Goal:** To find a function (often called a **policy**) that maps an observed **State** ($\\mathbf{s}$) of the environment to an optimal **Action** ($\\mathbf{a}$).\n",
    "* **State ($\\mathbf{s}$):** The agent's current situation or observation (e.g., a helicopter's position, orientation, and speed).\n",
    "* **Action ($\\mathbf{a}$):** A decision the agent makes (e.g., how to move the control sticks).\n",
    "* **Reward Function:** The key input to RL, which tells the agent *when* it is doing well and *when* it is doing poorly.\n",
    "    * **Incentive:** The agent's task is to figure out the sequence of actions that maximize the total cumulative reward over time.\n",
    "    * **Example:** For a helicopter, reward may be **+1** for every second flying well and a large **negative reward** (e.g., -1000) for crashing. \n",
    "\n",
    "<img src='images/rl.png' width='500px'>\n",
    "\n",
    "### The Power of Reward\n",
    "\n",
    "RL is powerful because the designer only needs to specify **what** the goal is (via the reward function), not **how** to achieve it (via specific optimal actions).\n",
    "* **Analogy:** It is like training a dog; you reward \"good dog\" behavior and discourage \"bad dog\" behavior, allowing the dog to learn the complex path to the desired outcome itself.\n",
    "* **Example:** An RL algorithm enabled a robot dog to learn complex leg placements to climb over obstacles solely by rewarding progress toward the goal, without explicit instructions on leg movement.\n",
    "\n",
    "### Contrast with Supervised Learning (SL)\n",
    "\n",
    "For many control tasks (like flying a robot), Supervised Learning (SL) fails because it requires a large dataset of states ($\\mathbf{x}$) and their ideal actions ($\\mathbf{y}$).\n",
    "\n",
    "It is often ambiguous or impossible for a human expert to define the single, exact \"right action\" ($\\mathbf{y}$) for every single complex state ($\\mathbf{x}$), making SL impractical for these scenarios. RL overcomes this ambiguity by using rewards instead of perfect labels.\n",
    "\n",
    "### Applications\n",
    "\n",
    "* **Robotics:** Controlling autonomous systems (helicopters, drones, robot dogs) to perform complex maneuvers.\n",
    "* **Optimization:** Factory optimization to maximize throughput and efficiency.\n",
    "* **Finance:** Efficient stock execution and trading strategies (e.g., sequencing trades to minimize price impact).\n",
    "* **Gaming:** Playing complex games like Chess, Go, Bridge, and various video games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bda744",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686110fa",
   "metadata": {},
   "source": [
    "## Mars rover example\n",
    "\n",
    "This section formalizes the core concepts of Reinforcement Learning (RL) using a simplified example inspired by the Mars rover, introducing the concepts of state, action, reward, and terminal states.\n",
    "\n",
    "### The Environment Setup (States and Rewards)\n",
    "\n",
    "* **States ($S$):** The environment is modeled as a sequence of six positions, $S_1$ through $S_6$, representing possible locations of the rover. The rover starts in $S_4$.\n",
    "* **Rewards ($R$):** Rewards are associated with specific states based on their scientific value:\n",
    "    * $R(S_1) = 100$ (Highest value, most interesting science).\n",
    "    * $R(S_6) = 40$ (Second highest value).\n",
    "    * $R(S_2) = R(S_3) = R(S_4) = R(S_5) = 0$.\n",
    "* **Terminal States:** $S_1$ and $S_6$ are terminal states. Once the rover reaches these states, the day (or episode) ends, and no further rewards can be earned.\n",
    "\n",
    "### Actions and Transitions\n",
    "\n",
    "* **Actions ($A$):** At each step, the rover can choose one of two actions:\n",
    "    * Go Left\n",
    "    * Go Right\n",
    "* **State Transition:** Taking an action leads the rover from the current state $S$ to a new state $S'$ (the next state). For example, from $S_4$, taking the action \"Go Left\" leads to the next state $S_3$.\n",
    "\n",
    "<img src='images/rl_example.png' width=600px>\n",
    "\n",
    "### The Core RL Loop Elements\n",
    "\n",
    "The fundamental process that defines the reinforcement learning problem is the sequence of transitions:\n",
    "\n",
    "At every time step, the robot is in a State ($\\mathbf{S}$), chooses an Action ($\\mathbf{A}$), receives the Reward ($\\mathbf{R}(S)$) associated with that state, and transitions to a Next State ($\\mathbf{S'}$).\n",
    "\n",
    "### Evaluating Action Sequences\n",
    "\n",
    "The goal of the RL algorithm is to figure out the optimal sequence of actions to maximize the total reward collected before reaching a terminal state.\n",
    "\n",
    "* **Option 1 (Go Left):** $S_4 \\to S_3 \\to S_2 \\to S_1$. Total Reward: $0 + 0 + 0 + 100 = 100$.\n",
    "* **Option 2 (Go Right):** $S_4 \\to S_5 \\to S_6$. Total Reward: $0 + 0 + 40 = 40$.\n",
    "* **Suboptimal Path:** $S_4 \\to S_5 \\to S_4 \\to \\dots$ (Wasting time by moving back and forth).\n",
    "\n",
    "The algorithm must learn to choose the path (policy) that yields the highest cumulative return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f54d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246384e",
   "metadata": {},
   "source": [
    "## The return in reinforcement learning\n",
    "\n",
    "The section explains the critical Reinforcement Learning (RL) concept of the **Return**, which is used to evaluate the desirability of a sequence of rewards by introducing a **Discount Factor ($\\gamma$)**.\n",
    "\n",
    "### Defining the Return ($G$)\n",
    "\n",
    "The Return is the single number used to quantify the total cumulative value of a sequence of rewards ($R_1, R_2, R_3, \\dots$) that an agent receives over an episode.\n",
    "* **Analogy:** It captures the idea that a smaller, immediate reward (like a \\$5 bill now) might be more attractive than a larger reward that takes significant time and effort to obtain (like a \\$10 bill across town).\n",
    "* **Goal:** The primary objective of an RL algorithm is to find a policy (set of actions) that maximizes the expected Return.\n",
    "\n",
    "### The Discount Factor ($\\gamma$)\n",
    "\n",
    "The discount factor, $\\gamma$ (gamma), is a number between 0 and 1 (often close to 1, like 0.9 or 0.99) used to weigh future rewards. It makes the RL algorithm **\"impatient\"** by reducing the value of rewards received later in time. Rewards received sooner contribute more to the total Return.\n",
    "\n",
    "### The Return Formula\n",
    "\n",
    "The return ($G$) is calculated as the sum of all future rewards, where each successive reward is discounted by an increasing power of $\\gamma$:\n",
    "\n",
    "$$G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\dots$$\n",
    "\n",
    "* The first reward ($R_1$) is given full credit ($1 \\cdot R_1$).\n",
    "* The second reward ($R_2$) is multiplied by $\\gamma$.\n",
    "* The third reward ($R_3$) is multiplied by $\\gamma^2$, and so on.\n",
    "\n",
    "### Interpretation and Practical Effects\n",
    "\n",
    "* **Financial Interpretation:** In applications like financial trading, $\\gamma$ often represents the time value of money or the interest rate, meaning a dollar today is worth more than a dollar in the future.\n",
    "* **Handling Negative Rewards:** If the system incurs negative rewards (costs or penalties), the discount factor incentivizes the algorithm to push these negative outcomes as far into the future as possible, minimizing their discounted impact on the total Return.\n",
    "* **Policy Dependence:** The Return obtained from any given state depends entirely on the actions (policy) the agent chooses. In the rover example, choosing \"Go Left\" yielded a higher Return (12.5) than choosing \"Go Right\" (10) from the starting state $S_4$.\n",
    "\n",
    "<img src='images/rl_return.png' width='700px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5accbf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e219ff2",
   "metadata": {},
   "source": [
    "## Making decisions: Policies in reinforcement learning\n",
    "\n",
    "This section explains the final formalized concept of Reinforcement Learning (RL): the **Policy ($\\pi$)**, which is the core output of any RL algorithm.\n",
    "\n",
    "### Definition of the Policy ($\\pi$)\n",
    "\n",
    "The policy ($\\pi$) is a function that serves as the \"brain\" or \"controller\" for the reinforcement learning agent. It takes the current State ($\\mathbf{s}$) as input and reliably outputs the recommended Action ($\\mathbf{a}$) that the agent should take in that state.\n",
    "\n",
    "$$\\pi(\\mathbf{s}) \\longrightarrow \\mathbf{a}$$\n",
    "\n",
    "### The Goal of Reinforcement Learning\n",
    "\n",
    "The ultimate objective of any reinforcement learning algorithm is to find the optimal policy ($\\pi^{*}$). This is the policy that, when followed, guarantees the maximum possible **Return** (the discounted sum of future rewards) for the agent from every starting state.\n",
    "\n",
    "### Policy Examples (Rover)\n",
    "\n",
    "A policy defines a decision for every possible state:\n",
    "\n",
    "*Example Policy:*\n",
    "* If in State 2, go **Left**.\n",
    "* If in State 3, go **Left**.\n",
    "* If in State 4, go **Left**.\n",
    "* If in State 5, go **Right**.\n",
    "\n",
    "The algorithm must explore and learn to define the best action for every state to maximize the cumulative rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503d3d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c349f",
   "metadata": {},
   "source": [
    "\n",
    "## Review of key concepts\n",
    "\n",
    "This section provides a concise review of the fundamental components of Reinforcement Learning (RL) using the formalism known as a **Markov Decision Process (MDP)**.\n",
    "\n",
    "### Core RL Components Reviewed\n",
    "\n",
    "The formalism requires defining five key elements for any application:\n",
    "\n",
    "* **States ($\\mathbf{S}$):** All possible configurations or situations of the environment (e.g., the rover's position, the helicopter's position/orientation, or the configuration of pieces on a chessboard).\n",
    "* **Actions ($\\mathbf{A}$):** The set of all possible decisions or moves the agent can make from any given state (e.g., \"go left/right\" for the rover, or moving a specific control stick for the helicopter).\n",
    "* **Rewards ($\\mathbf{R}$):** A function that assigns a positive or negative numerical value to a state or an action, telling the agent when it is performing well or poorly (e.g., $+1$ for winning a game, $-1$ for losing).\n",
    "* **Discount Factor ($\\gamma$):** A value (usually $0 < \\gamma < 1$) used to compute the Return, which makes the agent prioritize immediate rewards over future rewards.\n",
    "* **Return ($\\mathbf{G}$):** The cumulative discounted sum of all future rewards, which the policy attempts to maximize.\n",
    "* **Policy ($\\pi$):** The function that maps a State ($\\mathbf{S}$) to the optimal Action ($\\mathbf{A}$) to maximize the Return.\n",
    "\n",
    "### The Formalism: Markov Decision Process (MDP)\n",
    "\n",
    "The entire framework comprising the states, actions, transitions, and rewards is known as a **Markov Decision Process (MDP)**. The Markov term means that the future only depends on the current state and action, and not on the history of states or actions that led to the current state.  \n",
    "\n",
    "<img src='images/rl.png' width=500px>\n",
    "\n",
    "The MDP describes the interaction between the **Agent** and the **Environment**:\n",
    "1.  The Agent uses the **Policy ($\\pi$)** to choose an **Action ($\\mathbf{A}$)**.\n",
    "2.  The Action changes the **Environment**.\n",
    "3.  The Environment returns a new **State ($\\mathbf{S'}$)** and a **Reward ($\\mathbf{R}$)**.\n",
    "\n",
    "### Next Step: The State-Action Value Function\n",
    "\n",
    "In the next section, we will introduce the **State-Action Value Function** (or Q-function) as the next key concept necessary for developing algorithms to actually find the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430fedf5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01b7e0",
   "metadata": {},
   "source": [
    "## State-action value function definition\n",
    "\n",
    "This section introduces the final foundational concept needed for reinforcement learning algorithms: the **State-Action Value Function**, also known as the **Q-Function**.\n",
    "\n",
    "### Definition of the Q-Function\n",
    "\n",
    "The Q-Function is denoted by $Q(s, a)$. It gives a single number representing the expected total Return (discounted sum of future rewards) under a specific condition. $Q(s, a)$ is the Return you get if you **start in state $s$**, **take action $a$** just once, and then **behave optimally thereafter** (i.e., follow the optimal policy, $\\pi^*$). \n",
    "\n",
    "### The Relationship to Optimal Policy ($\\pi^*$)\n",
    "\n",
    "The initial definition is acknowledged as being slightly circular, as it requires knowledge of the \"optimal behavior\" to calculate $Q(s, a)$. Reinforcement learning algorithms resolve this by using techniques (like dynamic programming or temporal difference learning) to compute the $Q$-function *before* the optimal policy is known.\n",
    "\n",
    "### Using the Q-Function to Find the Optimal Action\n",
    "\n",
    "The Q-Function provides a direct way to find the optimal action to take in any state. The best possible Return an agent can get from state $s$ is the largest value of $Q(s, a)$ across all available actions $a$.\n",
    "    \n",
    "$$\\max_a Q(s, a)$$\n",
    "\n",
    "The optimal policy, $\\pi^*(s)$, simply chooses the action $a$ that maximizes $Q(s, a)$.\n",
    "\n",
    "$$\\pi^*(s) = \\underset{a}{\\operatorname{argmax}} \\ Q(s, a)$$\n",
    "\n",
    "### Example (Mars Rover, $\\gamma=0.5$)\n",
    "\n",
    "For the Mars Rover example, if $Q(S_4, \\text{Left}) = 12.5$ and $Q(S_4, \\text{Right}) = 10$:\n",
    "* The highest possible return from $S_4$ is $12.5$.\n",
    "* The optimal action in $S_4$ is **Go Left**, because that action yields the higher Q-value.\n",
    "\n",
    "<img src='images/Q-function.png' width='600px'>\n",
    "\n",
    "**Conclusion:** If an RL algorithm can successfully compute the $Q$-function for every state and every action, it has effectively solved the problem, as the optimal policy can be derived immediately by simply choosing the action with the highest Q-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e03fbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d17dad",
   "metadata": {},
   "source": [
    "## Bellman Equation\n",
    "\n",
    "This section introduces the **Bellman Equation**, the fundamental formula used in Reinforcement Learning (RL) to compute the **State-Action Value Function, $Q(s, a)$**.\n",
    "\n",
    "### The Goal and Definition\n",
    "\n",
    "The Bellman Equation is the key mathematical tool used to compute the $Q(s, a)$ values, which in turn are used to determine the optimal policy ($\\pi^*$). $Q(s, a)$ is the return if you start in state $s$, take action $a$ once, and then behave optimally afterward.\n",
    "\n",
    "### The Bellman Equation Formula\n",
    "\n",
    "The equation expresses the $Q(s, a)$ value recursively, breaking the total return into two parts: the immediate reward and the discounted optimal future return.\n",
    "\n",
    "$$\\mathbf{Q(s, a)} = \\mathbf{R(s)} + \\gamma \\cdot \\max_{a'} Q(s', a')$$\n",
    "\n",
    "Where:\n",
    "* **$s$:** The current state.\n",
    "* **$a$:** The current action taken.\n",
    "* **$R(s)$:** The immediate reward received in state $s$.\n",
    "* **$\\gamma$ (Gamma):** The discount factor.\n",
    "* **$s'$:** The next state reached after taking action $a$ from state $s$.\n",
    "* **$\\max_{a'} Q(s', a')$:** The maximum possible return obtainable from the *next* state, $s'$, by choosing the best possible subsequent action, $a'$.\n",
    "\n",
    "### Intuition and Breakdown\n",
    "\n",
    "The Bellman Equation formalizes the decomposition of the total Return:\n",
    "\n",
    "1.  **Immediate Reward ($R(s)$):** This is the reward you get right away at the first step.\n",
    "2.  **Discounted Future Return ($\\gamma \\cdot \\max_{a'} Q(s', a')$):** This is the value of the best possible return you can expect from the *next* state, $s'$, discounted by $\\gamma$.\n",
    "\n",
    "The value of taking an action now is equal to the reward you get now, plus the discounted value of the optimal returns you can expect from the state you land in next.\n",
    "\n",
    "### Edge Cases\n",
    "\n",
    "If $s$ is a terminal state (where the process ends), the Bellman Equation simplifies to $$Q(s, a) = R(s)$$ because there is no next state ($s'$), and therefore no future return.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "* Once this equation is defined, RL algorithms can be developed to iteratively solve or learn the $Q(s, a)$ values for all states and actions, despite the initial circular nature of the $Q$-function's definition.\n",
    "* The next section covers a topic on **Stochastic Markov Decision Processes**, where actions have random effects, before developing the first RL algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ca938",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
